{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "from typing import Any, Dict\n",
    "\n",
    "from einops import rearrange\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/changminjeon/.local/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-03-08 05:51:57.096121: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-03-08 05:51:57.165657: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-08 05:51:58.002267: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from ldm.models.autoencoder import AutoencoderKL  # First Stage\n",
    "from ldm.modules.encoders.modules import BERTEmbedder  # Condition Model\n",
    "from ldm.modules.diffusionmodules.openaimodel import UNetModel  # Diffusion Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LatentDiffusion(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        unet_config,\n",
    "        first_stage_config,\n",
    "        cond_stage_config,\n",
    "        scale_factor=1.0,\n",
    "        timesteps=1000,\n",
    "        linear_start=1e-4,\n",
    "        linear_end=2e-2,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.model = UNetModel(**unet_config)\n",
    "        self.first_stage_model = AutoencoderKL(**first_stage_config)\n",
    "        self.cond_stage_model = BERTEmbedder(**cond_stage_config)\n",
    "        self.scale_factor = scale_factor\n",
    "        self.num_timesteps = int(timesteps)\n",
    "\n",
    "        betas = (\n",
    "            np.linspace(linear_start**0.5, linear_end**0.5, timesteps, dtype=np.float64)\n",
    "            ** 2\n",
    "        )\n",
    "        alphas = 1.0 - betas\n",
    "        alphas_cumprod = np.cumprod(alphas, axis=0)\n",
    "        alphas_cumprod_prev = np.append(1.0, alphas_cumprod[:-1])\n",
    "        sqrt_one_minus_alphas_cumprod = np.sqrt(1.0 - alphas_cumprod)\n",
    "        assert timesteps == betas.shape[0] == alphas.shape[0] == alphas_cumprod.shape[0]\n",
    "\n",
    "        to_torch = lambda x: torch.tensor(x, dtype=torch.float32)\n",
    "        self.register_buffer(\"alphas_cumprod\", to_torch(alphas_cumprod))\n",
    "        self.register_buffer(\"alphas_cumprod_prev\", to_torch(alphas_cumprod_prev))\n",
    "        self.register_buffer(\n",
    "            \"sqrt_one_minus_alphas_cumprod\", to_torch(sqrt_one_minus_alphas_cumprod)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, t, cond):\n",
    "        return self.model(x, t, context=torch.cat([cond], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDIMSamper:\n",
    "    def __init__(self, model: LatentDiffusion):\n",
    "        self.model = model\n",
    "        self.ddpm_num_timesteps = model.num_timesteps\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def sample(\n",
    "        self,\n",
    "        ddim_steps,\n",
    "        batch_size,\n",
    "        img_shape,\n",
    "        conditioning=None,\n",
    "        unconditional_guidance_scale=1.0,\n",
    "        unconditional_conditioning=None,\n",
    "        temperature=1.0,\n",
    "    ):\n",
    "\n",
    "        device = self.model.alphas_cumprod.device\n",
    "        samples = torch.randn((batch_size, *img_shape), device=device)\n",
    "        timesteps = (\n",
    "            np.arange(0, self.ddpm_num_timesteps, self.ddpm_num_timesteps // ddim_steps)\n",
    "            + 1\n",
    "        )\n",
    "        iterator = tqdm(\n",
    "            np.flip(timesteps),\n",
    "            total=len(timesteps),\n",
    "        )\n",
    "        for i, step in enumerate(iterator):\n",
    "            timestamp = torch.full((batch_size,), step, device=device, dtype=torch.long)\n",
    "            index = self.ddpm_num_timesteps - i - 1\n",
    "            samples = self.sample_ddim(\n",
    "                samples,\n",
    "                conditioning,\n",
    "                timestamp,\n",
    "                index=index,\n",
    "                temperature=temperature,\n",
    "                unconditional_guidance_scale=unconditional_guidance_scale,\n",
    "                unconditional_conditioning=unconditional_conditioning,\n",
    "            )\n",
    "        return samples\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def sample_ddim(\n",
    "        self,\n",
    "        x,\n",
    "        c,\n",
    "        t,\n",
    "        index,\n",
    "        temperature=1.0,\n",
    "        unconditional_guidance_scale=1.0,\n",
    "        unconditional_conditioning=None,\n",
    "    ):\n",
    "        device = x.device\n",
    "        b = x.shape[0]\n",
    "\n",
    "        x_in = torch.cat([x] * 2)\n",
    "        t_in = torch.cat([t] * 2)\n",
    "        c_in = torch.cat([unconditional_conditioning, c])\n",
    "        e_t_uncond, e_t = self.model(x_in, t_in, c_in).chunk(2)\n",
    "        e_t = e_t_uncond + unconditional_guidance_scale * (e_t - e_t_uncond)\n",
    "\n",
    "        alphas = self.model.alphas_cumprod\n",
    "        alphas_prev = self.model.alphas_cumprod_prev\n",
    "        sqrt_one_minus_alphas = self.model.sqrt_one_minus_alphas_cumprod\n",
    "\n",
    "        # select parameters corresponding to the currently considered timestep\n",
    "        a_t = torch.full((b, 1, 1, 1), alphas[index], device=device)\n",
    "        a_prev = torch.full((b, 1, 1, 1), alphas_prev[index], device=device)\n",
    "        sigma_t = torch.full((b, 1, 1, 1), 0, device=device)\n",
    "        sqrt_one_minus_at = torch.full(\n",
    "            (b, 1, 1, 1), sqrt_one_minus_alphas[index], device=device\n",
    "        )\n",
    "\n",
    "        # current prediction for x_0\n",
    "        pred_x0 = (x - sqrt_one_minus_at * e_t) / a_t.sqrt()\n",
    "\n",
    "        # direction pointing to x_t\n",
    "        dir_xt = (1.0 - a_prev - sigma_t**2).sqrt() * e_t\n",
    "        noise = sigma_t * torch.randn(x.shape, device=device) * temperature\n",
    "        x_prev = a_prev.sqrt() * pred_x0 + dir_xt + noise\n",
    "        return x_prev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(H=256, W=256, ddim_steps=50, n_samples=4, outdir='outputs/txt2img-samples', prompt='a virus monster is playing guitar, oil on canvas', scale=5.0)\n"
     ]
    }
   ],
   "source": [
    "opt = Namespace()\n",
    "opt.prompt = \"a virus monster is playing guitar, oil on canvas\"\n",
    "opt.n_samples = 4\n",
    "opt.ddim_steps = 50\n",
    "opt.scale = 5.0\n",
    "opt.H = 256\n",
    "opt.W = 256\n",
    "opt.outdir = \"outputs/txt2img-samples\"\n",
    "print(opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cond_stage_config': {'n_embed': 1280, 'n_layer': 32},\n",
      " 'first_stage_config': {'ddconfig': {'attn_resolutions': [],\n",
      "                                     'ch': 128,\n",
      "                                     'ch_mult': [1, 2, 4, 4],\n",
      "                                     'double_z': True,\n",
      "                                     'dropout': 0.0,\n",
      "                                     'in_channels': 3,\n",
      "                                     'num_res_blocks': 2,\n",
      "                                     'out_ch': 3,\n",
      "                                     'resolution': 256,\n",
      "                                     'z_channels': 4},\n",
      "                        'embed_dim': 4,\n",
      "                        'lossconfig': {'target': 'torch.nn.Identity'},\n",
      "                        'monitor': 'val/rec_loss'},\n",
      " 'linear_end': 0.012,\n",
      " 'linear_start': 0.00085,\n",
      " 'scale_factor': 0.18215,\n",
      " 'timesteps': 1000,\n",
      " 'unet_config': {'attention_resolutions': [4, 2, 1],\n",
      "                 'channel_mult': [1, 2, 4, 4],\n",
      "                 'context_dim': 1280,\n",
      "                 'image_size': 32,\n",
      "                 'in_channels': 4,\n",
      "                 'legacy': False,\n",
      "                 'model_channels': 320,\n",
      "                 'num_heads': 8,\n",
      "                 'num_res_blocks': 2,\n",
      "                 'out_channels': 4,\n",
      "                 'transformer_depth': 1,\n",
      "                 'use_checkpoint': True,\n",
      "                 'use_spatial_transformer': True}}\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Working with z of shape (1, 4, 32, 32) = 4096 dimensions.\n",
      "making attention of type 'vanilla' with 512 in_channels\n"
     ]
    }
   ],
   "source": [
    "with Path(\"latent-diffusion.yaml\").open(\"r\") as f:\n",
    "    model_config = yaml.load(f, Loader=yaml.FullLoader)\n",
    "    pprint(model_config)\n",
    "model = LatentDiffusion(**model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def state_dict_of(state_dict_path: str) -> Dict[str, Any]:\n",
    "    state_dict = torch.load(state_dict_path, map_location=\"cpu\")[\"state_dict\"]\n",
    "    for k in list(state_dict.keys()):\n",
    "        if k.startswith(\"model.diffusion_model.\"):\n",
    "            new_k = k.replace(\"model.diffusion_model.\", \"model.\")\n",
    "            state_dict[new_k] = state_dict.pop(k)\n",
    "    return state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_keys, unexpected_keys = model.load_state_dict(\n",
    "    state_dict_of(\"models/ldm/text2img-large/model.ckpt\"), strict=False\n",
    ")\n",
    "assert not missing_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentDiffusion(\n",
       "  (model): UNetModel(\n",
       "    (time_embed): Sequential(\n",
       "      (0): Linear(in_features=320, out_features=1280, bias=True)\n",
       "      (1): SiLU()\n",
       "      (2): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "    )\n",
       "    (input_blocks): ModuleList(\n",
       "      (0): TimestepEmbedSequential(\n",
       "        (0): Conv2d(4, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (1-2): 2 x TimestepEmbedSequential(\n",
       "        (0): ResBlock(\n",
       "          (in_layers): Sequential(\n",
       "            (0): GroupNorm32(32, 320, eps=1e-05, affine=True)\n",
       "            (1): SiLU()\n",
       "            (2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "          (h_upd): Identity()\n",
       "          (x_upd): Identity()\n",
       "          (emb_layers): Sequential(\n",
       "            (0): SiLU()\n",
       "            (1): Linear(in_features=1280, out_features=320, bias=True)\n",
       "          )\n",
       "          (out_layers): Sequential(\n",
       "            (0): GroupNorm32(32, 320, eps=1e-05, affine=True)\n",
       "            (1): SiLU()\n",
       "            (2): Dropout(p=0, inplace=False)\n",
       "            (3): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "          (skip_connection): Identity()\n",
       "        )\n",
       "        (1): SpatialTransformer(\n",
       "          (norm): GroupNorm(32, 320, eps=1e-06, affine=True)\n",
       "          (proj_in): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (transformer_blocks): ModuleList(\n",
       "            (0): BasicTransformerBlock(\n",
       "              (attn1): CrossAttention(\n",
       "                (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
       "                (to_k): Linear(in_features=320, out_features=320, bias=False)\n",
       "                (to_v): Linear(in_features=320, out_features=320, bias=False)\n",
       "                (to_out): Sequential(\n",
       "                  (0): Linear(in_features=320, out_features=320, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (ff): FeedForward(\n",
       "                (net): Sequential(\n",
       "                  (0): GEGLU(\n",
       "                    (proj): Linear(in_features=320, out_features=2560, bias=True)\n",
       "                  )\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                  (2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "                )\n",
       "              )\n",
       "              (attn2): CrossAttention(\n",
       "                (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
       "                (to_k): Linear(in_features=1280, out_features=320, bias=False)\n",
       "                (to_v): Linear(in_features=1280, out_features=320, bias=False)\n",
       "                (to_out): Sequential(\n",
       "                  (0): Linear(in_features=320, out_features=320, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "              (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "              (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "          (proj_out): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (3): TimestepEmbedSequential(\n",
       "        (0): Downsample(\n",
       "          (op): Conv2d(320, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (4): TimestepEmbedSequential(\n",
       "        (0): ResBlock(\n",
       "          (in_layers): Sequential(\n",
       "            (0): GroupNorm32(32, 320, eps=1e-05, affine=True)\n",
       "            (1): SiLU()\n",
       "            (2): Conv2d(320, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "          (h_upd): Identity()\n",
       "          (x_upd): Identity()\n",
       "          (emb_layers): Sequential(\n",
       "            (0): SiLU()\n",
       "            (1): Linear(in_features=1280, out_features=640, bias=True)\n",
       "          )\n",
       "          (out_layers): Sequential(\n",
       "            (0): GroupNorm32(32, 640, eps=1e-05, affine=True)\n",
       "            (1): SiLU()\n",
       "            (2): Dropout(p=0, inplace=False)\n",
       "            (3): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "          (skip_connection): Conv2d(320, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (1): SpatialTransformer(\n",
       "          (norm): GroupNorm(32, 640, eps=1e-06, affine=True)\n",
       "          (proj_in): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (transformer_blocks): ModuleList(\n",
       "            (0): BasicTransformerBlock(\n",
       "              (attn1): CrossAttention(\n",
       "                (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
       "                (to_k): Linear(in_features=640, out_features=640, bias=False)\n",
       "                (to_v): Linear(in_features=640, out_features=640, bias=False)\n",
       "                (to_out): Sequential(\n",
       "                  (0): Linear(in_features=640, out_features=640, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (ff): FeedForward(\n",
       "                (net): Sequential(\n",
       "                  (0): GEGLU(\n",
       "                    (proj): Linear(in_features=640, out_features=5120, bias=True)\n",
       "                  )\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                  (2): Linear(in_features=2560, out_features=640, bias=True)\n",
       "                )\n",
       "              )\n",
       "              (attn2): CrossAttention(\n",
       "                (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
       "                (to_k): Linear(in_features=1280, out_features=640, bias=False)\n",
       "                (to_v): Linear(in_features=1280, out_features=640, bias=False)\n",
       "                (to_out): Sequential(\n",
       "                  (0): Linear(in_features=640, out_features=640, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "              (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "              (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "          (proj_out): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (5): TimestepEmbedSequential(\n",
       "        (0): ResBlock(\n",
       "          (in_layers): Sequential(\n",
       "            (0): GroupNorm32(32, 640, eps=1e-05, affine=True)\n",
       "            (1): SiLU()\n",
       "            (2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "          (h_upd): Identity()\n",
       "          (x_upd): Identity()\n",
       "          (emb_layers): Sequential(\n",
       "            (0): SiLU()\n",
       "            (1): Linear(in_features=1280, out_features=640, bias=True)\n",
       "          )\n",
       "          (out_layers): Sequential(\n",
       "            (0): GroupNorm32(32, 640, eps=1e-05, affine=True)\n",
       "            (1): SiLU()\n",
       "            (2): Dropout(p=0, inplace=False)\n",
       "            (3): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "          (skip_connection): Identity()\n",
       "        )\n",
       "        (1): SpatialTransformer(\n",
       "          (norm): GroupNorm(32, 640, eps=1e-06, affine=True)\n",
       "          (proj_in): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (transformer_blocks): ModuleList(\n",
       "            (0): BasicTransformerBlock(\n",
       "              (attn1): CrossAttention(\n",
       "                (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
       "                (to_k): Linear(in_features=640, out_features=640, bias=False)\n",
       "                (to_v): Linear(in_features=640, out_features=640, bias=False)\n",
       "                (to_out): Sequential(\n",
       "                  (0): Linear(in_features=640, out_features=640, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (ff): FeedForward(\n",
       "                (net): Sequential(\n",
       "                  (0): GEGLU(\n",
       "                    (proj): Linear(in_features=640, out_features=5120, bias=True)\n",
       "                  )\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                  (2): Linear(in_features=2560, out_features=640, bias=True)\n",
       "                )\n",
       "              )\n",
       "              (attn2): CrossAttention(\n",
       "                (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
       "                (to_k): Linear(in_features=1280, out_features=640, bias=False)\n",
       "                (to_v): Linear(in_features=1280, out_features=640, bias=False)\n",
       "                (to_out): Sequential(\n",
       "                  (0): Linear(in_features=640, out_features=640, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "              (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "              (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "          (proj_out): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (6): TimestepEmbedSequential(\n",
       "        (0): Downsample(\n",
       "          (op): Conv2d(640, 640, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (7): TimestepEmbedSequential(\n",
       "        (0): ResBlock(\n",
       "          (in_layers): Sequential(\n",
       "            (0): GroupNorm32(32, 640, eps=1e-05, affine=True)\n",
       "            (1): SiLU()\n",
       "            (2): Conv2d(640, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "          (h_upd): Identity()\n",
       "          (x_upd): Identity()\n",
       "          (emb_layers): Sequential(\n",
       "            (0): SiLU()\n",
       "            (1): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          )\n",
       "          (out_layers): Sequential(\n",
       "            (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)\n",
       "            (1): SiLU()\n",
       "            (2): Dropout(p=0, inplace=False)\n",
       "            (3): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "          (skip_connection): Conv2d(640, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (1): SpatialTransformer(\n",
       "          (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
       "          (proj_in): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (transformer_blocks): ModuleList(\n",
       "            (0): BasicTransformerBlock(\n",
       "              (attn1): CrossAttention(\n",
       "                (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                (to_out): Sequential(\n",
       "                  (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (ff): FeedForward(\n",
       "                (net): Sequential(\n",
       "                  (0): GEGLU(\n",
       "                    (proj): Linear(in_features=1280, out_features=10240, bias=True)\n",
       "                  )\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                  (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "                )\n",
       "              )\n",
       "              (attn2): CrossAttention(\n",
       "                (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                (to_out): Sequential(\n",
       "                  (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "              (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "              (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "          (proj_out): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (8): TimestepEmbedSequential(\n",
       "        (0): ResBlock(\n",
       "          (in_layers): Sequential(\n",
       "            (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)\n",
       "            (1): SiLU()\n",
       "            (2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "          (h_upd): Identity()\n",
       "          (x_upd): Identity()\n",
       "          (emb_layers): Sequential(\n",
       "            (0): SiLU()\n",
       "            (1): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          )\n",
       "          (out_layers): Sequential(\n",
       "            (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)\n",
       "            (1): SiLU()\n",
       "            (2): Dropout(p=0, inplace=False)\n",
       "            (3): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "          (skip_connection): Identity()\n",
       "        )\n",
       "        (1): SpatialTransformer(\n",
       "          (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
       "          (proj_in): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (transformer_blocks): ModuleList(\n",
       "            (0): BasicTransformerBlock(\n",
       "              (attn1): CrossAttention(\n",
       "                (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                (to_out): Sequential(\n",
       "                  (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (ff): FeedForward(\n",
       "                (net): Sequential(\n",
       "                  (0): GEGLU(\n",
       "                    (proj): Linear(in_features=1280, out_features=10240, bias=True)\n",
       "                  )\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                  (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "                )\n",
       "              )\n",
       "              (attn2): CrossAttention(\n",
       "                (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                (to_out): Sequential(\n",
       "                  (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "              (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "              (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "          (proj_out): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (9): TimestepEmbedSequential(\n",
       "        (0): Downsample(\n",
       "          (op): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (10-11): 2 x TimestepEmbedSequential(\n",
       "        (0): ResBlock(\n",
       "          (in_layers): Sequential(\n",
       "            (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)\n",
       "            (1): SiLU()\n",
       "            (2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "          (h_upd): Identity()\n",
       "          (x_upd): Identity()\n",
       "          (emb_layers): Sequential(\n",
       "            (0): SiLU()\n",
       "            (1): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          )\n",
       "          (out_layers): Sequential(\n",
       "            (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)\n",
       "            (1): SiLU()\n",
       "            (2): Dropout(p=0, inplace=False)\n",
       "            (3): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "          (skip_connection): Identity()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (middle_block): TimestepEmbedSequential(\n",
       "      (0): ResBlock(\n",
       "        (in_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (h_upd): Identity()\n",
       "        (x_upd): Identity()\n",
       "        (emb_layers): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (out_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Dropout(p=0, inplace=False)\n",
       "          (3): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (skip_connection): Identity()\n",
       "      )\n",
       "      (1): SpatialTransformer(\n",
       "        (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
       "        (proj_in): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (transformer_blocks): ModuleList(\n",
       "          (0): BasicTransformerBlock(\n",
       "            (attn1): CrossAttention(\n",
       "              (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "              (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "              (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "              (to_out): Sequential(\n",
       "                (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (ff): FeedForward(\n",
       "              (net): Sequential(\n",
       "                (0): GEGLU(\n",
       "                  (proj): Linear(in_features=1280, out_features=10240, bias=True)\n",
       "                )\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "                (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (attn2): CrossAttention(\n",
       "              (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "              (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "              (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "              (to_out): Sequential(\n",
       "                (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (proj_out): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (2): ResBlock(\n",
       "        (in_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (h_upd): Identity()\n",
       "        (x_upd): Identity()\n",
       "        (emb_layers): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (out_layers): Sequential(\n",
       "          (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Dropout(p=0, inplace=False)\n",
       "          (3): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (skip_connection): Identity()\n",
       "      )\n",
       "    )\n",
       "    (output_blocks): ModuleList(\n",
       "      (0-1): 2 x TimestepEmbedSequential(\n",
       "        (0): ResBlock(\n",
       "          (in_layers): Sequential(\n",
       "            (0): GroupNorm32(32, 2560, eps=1e-05, affine=True)\n",
       "            (1): SiLU()\n",
       "            (2): Conv2d(2560, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "          (h_upd): Identity()\n",
       "          (x_upd): Identity()\n",
       "          (emb_layers): Sequential(\n",
       "            (0): SiLU()\n",
       "            (1): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          )\n",
       "          (out_layers): Sequential(\n",
       "            (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)\n",
       "            (1): SiLU()\n",
       "            (2): Dropout(p=0, inplace=False)\n",
       "            (3): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "          (skip_connection): Conv2d(2560, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (2): TimestepEmbedSequential(\n",
       "        (0): ResBlock(\n",
       "          (in_layers): Sequential(\n",
       "            (0): GroupNorm32(32, 2560, eps=1e-05, affine=True)\n",
       "            (1): SiLU()\n",
       "            (2): Conv2d(2560, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "          (h_upd): Identity()\n",
       "          (x_upd): Identity()\n",
       "          (emb_layers): Sequential(\n",
       "            (0): SiLU()\n",
       "            (1): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          )\n",
       "          (out_layers): Sequential(\n",
       "            (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)\n",
       "            (1): SiLU()\n",
       "            (2): Dropout(p=0, inplace=False)\n",
       "            (3): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "          (skip_connection): Conv2d(2560, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (1): Upsample(\n",
       "          (conv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (3-4): 2 x TimestepEmbedSequential(\n",
       "        (0): ResBlock(\n",
       "          (in_layers): Sequential(\n",
       "            (0): GroupNorm32(32, 2560, eps=1e-05, affine=True)\n",
       "            (1): SiLU()\n",
       "            (2): Conv2d(2560, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "          (h_upd): Identity()\n",
       "          (x_upd): Identity()\n",
       "          (emb_layers): Sequential(\n",
       "            (0): SiLU()\n",
       "            (1): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          )\n",
       "          (out_layers): Sequential(\n",
       "            (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)\n",
       "            (1): SiLU()\n",
       "            (2): Dropout(p=0, inplace=False)\n",
       "            (3): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "          (skip_connection): Conv2d(2560, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (1): SpatialTransformer(\n",
       "          (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
       "          (proj_in): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (transformer_blocks): ModuleList(\n",
       "            (0): BasicTransformerBlock(\n",
       "              (attn1): CrossAttention(\n",
       "                (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                (to_out): Sequential(\n",
       "                  (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (ff): FeedForward(\n",
       "                (net): Sequential(\n",
       "                  (0): GEGLU(\n",
       "                    (proj): Linear(in_features=1280, out_features=10240, bias=True)\n",
       "                  )\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                  (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "                )\n",
       "              )\n",
       "              (attn2): CrossAttention(\n",
       "                (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                (to_out): Sequential(\n",
       "                  (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "              (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "              (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "          (proj_out): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (5): TimestepEmbedSequential(\n",
       "        (0): ResBlock(\n",
       "          (in_layers): Sequential(\n",
       "            (0): GroupNorm32(32, 1920, eps=1e-05, affine=True)\n",
       "            (1): SiLU()\n",
       "            (2): Conv2d(1920, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "          (h_upd): Identity()\n",
       "          (x_upd): Identity()\n",
       "          (emb_layers): Sequential(\n",
       "            (0): SiLU()\n",
       "            (1): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          )\n",
       "          (out_layers): Sequential(\n",
       "            (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)\n",
       "            (1): SiLU()\n",
       "            (2): Dropout(p=0, inplace=False)\n",
       "            (3): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "          (skip_connection): Conv2d(1920, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (1): SpatialTransformer(\n",
       "          (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
       "          (proj_in): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (transformer_blocks): ModuleList(\n",
       "            (0): BasicTransformerBlock(\n",
       "              (attn1): CrossAttention(\n",
       "                (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                (to_out): Sequential(\n",
       "                  (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (ff): FeedForward(\n",
       "                (net): Sequential(\n",
       "                  (0): GEGLU(\n",
       "                    (proj): Linear(in_features=1280, out_features=10240, bias=True)\n",
       "                  )\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                  (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "                )\n",
       "              )\n",
       "              (attn2): CrossAttention(\n",
       "                (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                (to_out): Sequential(\n",
       "                  (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "              (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "              (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "          (proj_out): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (2): Upsample(\n",
       "          (conv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (6): TimestepEmbedSequential(\n",
       "        (0): ResBlock(\n",
       "          (in_layers): Sequential(\n",
       "            (0): GroupNorm32(32, 1920, eps=1e-05, affine=True)\n",
       "            (1): SiLU()\n",
       "            (2): Conv2d(1920, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "          (h_upd): Identity()\n",
       "          (x_upd): Identity()\n",
       "          (emb_layers): Sequential(\n",
       "            (0): SiLU()\n",
       "            (1): Linear(in_features=1280, out_features=640, bias=True)\n",
       "          )\n",
       "          (out_layers): Sequential(\n",
       "            (0): GroupNorm32(32, 640, eps=1e-05, affine=True)\n",
       "            (1): SiLU()\n",
       "            (2): Dropout(p=0, inplace=False)\n",
       "            (3): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "          (skip_connection): Conv2d(1920, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (1): SpatialTransformer(\n",
       "          (norm): GroupNorm(32, 640, eps=1e-06, affine=True)\n",
       "          (proj_in): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (transformer_blocks): ModuleList(\n",
       "            (0): BasicTransformerBlock(\n",
       "              (attn1): CrossAttention(\n",
       "                (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
       "                (to_k): Linear(in_features=640, out_features=640, bias=False)\n",
       "                (to_v): Linear(in_features=640, out_features=640, bias=False)\n",
       "                (to_out): Sequential(\n",
       "                  (0): Linear(in_features=640, out_features=640, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (ff): FeedForward(\n",
       "                (net): Sequential(\n",
       "                  (0): GEGLU(\n",
       "                    (proj): Linear(in_features=640, out_features=5120, bias=True)\n",
       "                  )\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                  (2): Linear(in_features=2560, out_features=640, bias=True)\n",
       "                )\n",
       "              )\n",
       "              (attn2): CrossAttention(\n",
       "                (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
       "                (to_k): Linear(in_features=1280, out_features=640, bias=False)\n",
       "                (to_v): Linear(in_features=1280, out_features=640, bias=False)\n",
       "                (to_out): Sequential(\n",
       "                  (0): Linear(in_features=640, out_features=640, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "              (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "              (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "          (proj_out): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (7): TimestepEmbedSequential(\n",
       "        (0): ResBlock(\n",
       "          (in_layers): Sequential(\n",
       "            (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)\n",
       "            (1): SiLU()\n",
       "            (2): Conv2d(1280, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "          (h_upd): Identity()\n",
       "          (x_upd): Identity()\n",
       "          (emb_layers): Sequential(\n",
       "            (0): SiLU()\n",
       "            (1): Linear(in_features=1280, out_features=640, bias=True)\n",
       "          )\n",
       "          (out_layers): Sequential(\n",
       "            (0): GroupNorm32(32, 640, eps=1e-05, affine=True)\n",
       "            (1): SiLU()\n",
       "            (2): Dropout(p=0, inplace=False)\n",
       "            (3): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "          (skip_connection): Conv2d(1280, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (1): SpatialTransformer(\n",
       "          (norm): GroupNorm(32, 640, eps=1e-06, affine=True)\n",
       "          (proj_in): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (transformer_blocks): ModuleList(\n",
       "            (0): BasicTransformerBlock(\n",
       "              (attn1): CrossAttention(\n",
       "                (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
       "                (to_k): Linear(in_features=640, out_features=640, bias=False)\n",
       "                (to_v): Linear(in_features=640, out_features=640, bias=False)\n",
       "                (to_out): Sequential(\n",
       "                  (0): Linear(in_features=640, out_features=640, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (ff): FeedForward(\n",
       "                (net): Sequential(\n",
       "                  (0): GEGLU(\n",
       "                    (proj): Linear(in_features=640, out_features=5120, bias=True)\n",
       "                  )\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                  (2): Linear(in_features=2560, out_features=640, bias=True)\n",
       "                )\n",
       "              )\n",
       "              (attn2): CrossAttention(\n",
       "                (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
       "                (to_k): Linear(in_features=1280, out_features=640, bias=False)\n",
       "                (to_v): Linear(in_features=1280, out_features=640, bias=False)\n",
       "                (to_out): Sequential(\n",
       "                  (0): Linear(in_features=640, out_features=640, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "              (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "              (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "          (proj_out): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (8): TimestepEmbedSequential(\n",
       "        (0): ResBlock(\n",
       "          (in_layers): Sequential(\n",
       "            (0): GroupNorm32(32, 960, eps=1e-05, affine=True)\n",
       "            (1): SiLU()\n",
       "            (2): Conv2d(960, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "          (h_upd): Identity()\n",
       "          (x_upd): Identity()\n",
       "          (emb_layers): Sequential(\n",
       "            (0): SiLU()\n",
       "            (1): Linear(in_features=1280, out_features=640, bias=True)\n",
       "          )\n",
       "          (out_layers): Sequential(\n",
       "            (0): GroupNorm32(32, 640, eps=1e-05, affine=True)\n",
       "            (1): SiLU()\n",
       "            (2): Dropout(p=0, inplace=False)\n",
       "            (3): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "          (skip_connection): Conv2d(960, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (1): SpatialTransformer(\n",
       "          (norm): GroupNorm(32, 640, eps=1e-06, affine=True)\n",
       "          (proj_in): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (transformer_blocks): ModuleList(\n",
       "            (0): BasicTransformerBlock(\n",
       "              (attn1): CrossAttention(\n",
       "                (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
       "                (to_k): Linear(in_features=640, out_features=640, bias=False)\n",
       "                (to_v): Linear(in_features=640, out_features=640, bias=False)\n",
       "                (to_out): Sequential(\n",
       "                  (0): Linear(in_features=640, out_features=640, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (ff): FeedForward(\n",
       "                (net): Sequential(\n",
       "                  (0): GEGLU(\n",
       "                    (proj): Linear(in_features=640, out_features=5120, bias=True)\n",
       "                  )\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                  (2): Linear(in_features=2560, out_features=640, bias=True)\n",
       "                )\n",
       "              )\n",
       "              (attn2): CrossAttention(\n",
       "                (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
       "                (to_k): Linear(in_features=1280, out_features=640, bias=False)\n",
       "                (to_v): Linear(in_features=1280, out_features=640, bias=False)\n",
       "                (to_out): Sequential(\n",
       "                  (0): Linear(in_features=640, out_features=640, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "              (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "              (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "          (proj_out): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (2): Upsample(\n",
       "          (conv): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (9): TimestepEmbedSequential(\n",
       "        (0): ResBlock(\n",
       "          (in_layers): Sequential(\n",
       "            (0): GroupNorm32(32, 960, eps=1e-05, affine=True)\n",
       "            (1): SiLU()\n",
       "            (2): Conv2d(960, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "          (h_upd): Identity()\n",
       "          (x_upd): Identity()\n",
       "          (emb_layers): Sequential(\n",
       "            (0): SiLU()\n",
       "            (1): Linear(in_features=1280, out_features=320, bias=True)\n",
       "          )\n",
       "          (out_layers): Sequential(\n",
       "            (0): GroupNorm32(32, 320, eps=1e-05, affine=True)\n",
       "            (1): SiLU()\n",
       "            (2): Dropout(p=0, inplace=False)\n",
       "            (3): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "          (skip_connection): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (1): SpatialTransformer(\n",
       "          (norm): GroupNorm(32, 320, eps=1e-06, affine=True)\n",
       "          (proj_in): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (transformer_blocks): ModuleList(\n",
       "            (0): BasicTransformerBlock(\n",
       "              (attn1): CrossAttention(\n",
       "                (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
       "                (to_k): Linear(in_features=320, out_features=320, bias=False)\n",
       "                (to_v): Linear(in_features=320, out_features=320, bias=False)\n",
       "                (to_out): Sequential(\n",
       "                  (0): Linear(in_features=320, out_features=320, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (ff): FeedForward(\n",
       "                (net): Sequential(\n",
       "                  (0): GEGLU(\n",
       "                    (proj): Linear(in_features=320, out_features=2560, bias=True)\n",
       "                  )\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                  (2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "                )\n",
       "              )\n",
       "              (attn2): CrossAttention(\n",
       "                (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
       "                (to_k): Linear(in_features=1280, out_features=320, bias=False)\n",
       "                (to_v): Linear(in_features=1280, out_features=320, bias=False)\n",
       "                (to_out): Sequential(\n",
       "                  (0): Linear(in_features=320, out_features=320, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "              (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "              (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "          (proj_out): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (10-11): 2 x TimestepEmbedSequential(\n",
       "        (0): ResBlock(\n",
       "          (in_layers): Sequential(\n",
       "            (0): GroupNorm32(32, 640, eps=1e-05, affine=True)\n",
       "            (1): SiLU()\n",
       "            (2): Conv2d(640, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "          (h_upd): Identity()\n",
       "          (x_upd): Identity()\n",
       "          (emb_layers): Sequential(\n",
       "            (0): SiLU()\n",
       "            (1): Linear(in_features=1280, out_features=320, bias=True)\n",
       "          )\n",
       "          (out_layers): Sequential(\n",
       "            (0): GroupNorm32(32, 320, eps=1e-05, affine=True)\n",
       "            (1): SiLU()\n",
       "            (2): Dropout(p=0, inplace=False)\n",
       "            (3): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "          (skip_connection): Conv2d(640, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (1): SpatialTransformer(\n",
       "          (norm): GroupNorm(32, 320, eps=1e-06, affine=True)\n",
       "          (proj_in): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (transformer_blocks): ModuleList(\n",
       "            (0): BasicTransformerBlock(\n",
       "              (attn1): CrossAttention(\n",
       "                (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
       "                (to_k): Linear(in_features=320, out_features=320, bias=False)\n",
       "                (to_v): Linear(in_features=320, out_features=320, bias=False)\n",
       "                (to_out): Sequential(\n",
       "                  (0): Linear(in_features=320, out_features=320, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (ff): FeedForward(\n",
       "                (net): Sequential(\n",
       "                  (0): GEGLU(\n",
       "                    (proj): Linear(in_features=320, out_features=2560, bias=True)\n",
       "                  )\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                  (2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "                )\n",
       "              )\n",
       "              (attn2): CrossAttention(\n",
       "                (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
       "                (to_k): Linear(in_features=1280, out_features=320, bias=False)\n",
       "                (to_v): Linear(in_features=1280, out_features=320, bias=False)\n",
       "                (to_out): Sequential(\n",
       "                  (0): Linear(in_features=320, out_features=320, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "              (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "              (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "          (proj_out): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (out): Sequential(\n",
       "      (0): GroupNorm32(32, 320, eps=1e-05, affine=True)\n",
       "      (1): SiLU()\n",
       "      (2): Conv2d(320, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (first_stage_model): AutoencoderKL(\n",
       "    (encoder): Encoder(\n",
       "      (conv_in): Conv2d(3, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (down): ModuleList(\n",
       "        (0): Module(\n",
       "          (block): ModuleList(\n",
       "            (0-1): 2 x ResnetBlock(\n",
       "              (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            )\n",
       "          )\n",
       "          (attn): ModuleList()\n",
       "          (downsample): Downsample(\n",
       "            (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2))\n",
       "          )\n",
       "        )\n",
       "        (1): Module(\n",
       "          (block): ModuleList(\n",
       "            (0): ResnetBlock(\n",
       "              (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "              (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (nin_shortcut): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "            (1): ResnetBlock(\n",
       "              (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            )\n",
       "          )\n",
       "          (attn): ModuleList()\n",
       "          (downsample): Downsample(\n",
       "            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))\n",
       "          )\n",
       "        )\n",
       "        (2): Module(\n",
       "          (block): ModuleList(\n",
       "            (0): ResnetBlock(\n",
       "              (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "              (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (nin_shortcut): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "            (1): ResnetBlock(\n",
       "              (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "              (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            )\n",
       "          )\n",
       "          (attn): ModuleList()\n",
       "          (downsample): Downsample(\n",
       "            (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2))\n",
       "          )\n",
       "        )\n",
       "        (3): Module(\n",
       "          (block): ModuleList(\n",
       "            (0-1): 2 x ResnetBlock(\n",
       "              (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "              (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            )\n",
       "          )\n",
       "          (attn): ModuleList()\n",
       "        )\n",
       "      )\n",
       "      (mid): Module(\n",
       "        (block_1): ResnetBlock(\n",
       "          (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (attn_1): AttnBlock(\n",
       "          (norm): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "          (q): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (k): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (v): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (proj_out): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (block_2): ResnetBlock(\n",
       "          (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (norm_out): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "      (conv_out): Conv2d(512, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "    (decoder): Decoder(\n",
       "      (conv_in): Conv2d(4, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (mid): Module(\n",
       "        (block_1): ResnetBlock(\n",
       "          (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (attn_1): AttnBlock(\n",
       "          (norm): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "          (q): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (k): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (v): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (proj_out): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (block_2): ResnetBlock(\n",
       "          (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (up): ModuleList(\n",
       "        (0): Module(\n",
       "          (block): ModuleList(\n",
       "            (0): ResnetBlock(\n",
       "              (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "              (conv1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (nin_shortcut): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "            (1-2): 2 x ResnetBlock(\n",
       "              (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            )\n",
       "          )\n",
       "          (attn): ModuleList()\n",
       "        )\n",
       "        (1): Module(\n",
       "          (block): ModuleList(\n",
       "            (0): ResnetBlock(\n",
       "              (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "              (conv1): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (nin_shortcut): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "            (1-2): 2 x ResnetBlock(\n",
       "              (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            )\n",
       "          )\n",
       "          (attn): ModuleList()\n",
       "          (upsample): Upsample(\n",
       "            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (2-3): 2 x Module(\n",
       "          (block): ModuleList(\n",
       "            (0-2): 3 x ResnetBlock(\n",
       "              (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "              (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            )\n",
       "          )\n",
       "          (attn): ModuleList()\n",
       "          (upsample): Upsample(\n",
       "            (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (norm_out): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "      (conv_out): Conv2d(128, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "    (loss): Identity()\n",
       "    (quant_conv): Conv2d(8, 8, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (post_quant_conv): Conv2d(4, 4, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       "  (cond_stage_model): BERTEmbedder(\n",
       "    (tknz_fn): BERTTokenizer()\n",
       "    (transformer): TransformerWrapper(\n",
       "      (token_emb): Embedding(30522, 1280)\n",
       "      (pos_emb): AbsolutePositionalEmbedding(\n",
       "        (emb): Embedding(77, 1280)\n",
       "      )\n",
       "      (emb_dropout): Dropout(p=0.0, inplace=False)\n",
       "      (project_emb): Identity()\n",
       "      (attn_layers): Encoder(\n",
       "        (layers): ModuleList(\n",
       "          (0): ModuleList(\n",
       "            (0): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Attention(\n",
       "              (to_q): Linear(in_features=1280, out_features=512, bias=False)\n",
       "              (to_k): Linear(in_features=1280, out_features=512, bias=False)\n",
       "              (to_v): Linear(in_features=1280, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (to_out): Linear(in_features=512, out_features=1280, bias=True)\n",
       "            )\n",
       "            (2): Residual()\n",
       "          )\n",
       "          (1): ModuleList(\n",
       "            (0): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): FeedForward(\n",
       "              (net): Sequential(\n",
       "                (0): Sequential(\n",
       "                  (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "                  (1): GELU(approximate='none')\n",
       "                )\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "                (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (2): Residual()\n",
       "          )\n",
       "          (2): ModuleList(\n",
       "            (0): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Attention(\n",
       "              (to_q): Linear(in_features=1280, out_features=512, bias=False)\n",
       "              (to_k): Linear(in_features=1280, out_features=512, bias=False)\n",
       "              (to_v): Linear(in_features=1280, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (to_out): Linear(in_features=512, out_features=1280, bias=True)\n",
       "            )\n",
       "            (2): Residual()\n",
       "          )\n",
       "          (3): ModuleList(\n",
       "            (0): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): FeedForward(\n",
       "              (net): Sequential(\n",
       "                (0): Sequential(\n",
       "                  (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "                  (1): GELU(approximate='none')\n",
       "                )\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "                (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (2): Residual()\n",
       "          )\n",
       "          (4): ModuleList(\n",
       "            (0): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Attention(\n",
       "              (to_q): Linear(in_features=1280, out_features=512, bias=False)\n",
       "              (to_k): Linear(in_features=1280, out_features=512, bias=False)\n",
       "              (to_v): Linear(in_features=1280, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (to_out): Linear(in_features=512, out_features=1280, bias=True)\n",
       "            )\n",
       "            (2): Residual()\n",
       "          )\n",
       "          (5): ModuleList(\n",
       "            (0): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): FeedForward(\n",
       "              (net): Sequential(\n",
       "                (0): Sequential(\n",
       "                  (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "                  (1): GELU(approximate='none')\n",
       "                )\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "                (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (2): Residual()\n",
       "          )\n",
       "          (6): ModuleList(\n",
       "            (0): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Attention(\n",
       "              (to_q): Linear(in_features=1280, out_features=512, bias=False)\n",
       "              (to_k): Linear(in_features=1280, out_features=512, bias=False)\n",
       "              (to_v): Linear(in_features=1280, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (to_out): Linear(in_features=512, out_features=1280, bias=True)\n",
       "            )\n",
       "            (2): Residual()\n",
       "          )\n",
       "          (7): ModuleList(\n",
       "            (0): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): FeedForward(\n",
       "              (net): Sequential(\n",
       "                (0): Sequential(\n",
       "                  (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "                  (1): GELU(approximate='none')\n",
       "                )\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "                (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (2): Residual()\n",
       "          )\n",
       "          (8): ModuleList(\n",
       "            (0): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Attention(\n",
       "              (to_q): Linear(in_features=1280, out_features=512, bias=False)\n",
       "              (to_k): Linear(in_features=1280, out_features=512, bias=False)\n",
       "              (to_v): Linear(in_features=1280, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (to_out): Linear(in_features=512, out_features=1280, bias=True)\n",
       "            )\n",
       "            (2): Residual()\n",
       "          )\n",
       "          (9): ModuleList(\n",
       "            (0): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): FeedForward(\n",
       "              (net): Sequential(\n",
       "                (0): Sequential(\n",
       "                  (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "                  (1): GELU(approximate='none')\n",
       "                )\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "                (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (2): Residual()\n",
       "          )\n",
       "          (10): ModuleList(\n",
       "            (0): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Attention(\n",
       "              (to_q): Linear(in_features=1280, out_features=512, bias=False)\n",
       "              (to_k): Linear(in_features=1280, out_features=512, bias=False)\n",
       "              (to_v): Linear(in_features=1280, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (to_out): Linear(in_features=512, out_features=1280, bias=True)\n",
       "            )\n",
       "            (2): Residual()\n",
       "          )\n",
       "          (11): ModuleList(\n",
       "            (0): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): FeedForward(\n",
       "              (net): Sequential(\n",
       "                (0): Sequential(\n",
       "                  (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "                  (1): GELU(approximate='none')\n",
       "                )\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "                (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (2): Residual()\n",
       "          )\n",
       "          (12): ModuleList(\n",
       "            (0): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Attention(\n",
       "              (to_q): Linear(in_features=1280, out_features=512, bias=False)\n",
       "              (to_k): Linear(in_features=1280, out_features=512, bias=False)\n",
       "              (to_v): Linear(in_features=1280, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (to_out): Linear(in_features=512, out_features=1280, bias=True)\n",
       "            )\n",
       "            (2): Residual()\n",
       "          )\n",
       "          (13): ModuleList(\n",
       "            (0): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): FeedForward(\n",
       "              (net): Sequential(\n",
       "                (0): Sequential(\n",
       "                  (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "                  (1): GELU(approximate='none')\n",
       "                )\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "                (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (2): Residual()\n",
       "          )\n",
       "          (14): ModuleList(\n",
       "            (0): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Attention(\n",
       "              (to_q): Linear(in_features=1280, out_features=512, bias=False)\n",
       "              (to_k): Linear(in_features=1280, out_features=512, bias=False)\n",
       "              (to_v): Linear(in_features=1280, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (to_out): Linear(in_features=512, out_features=1280, bias=True)\n",
       "            )\n",
       "            (2): Residual()\n",
       "          )\n",
       "          (15): ModuleList(\n",
       "            (0): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): FeedForward(\n",
       "              (net): Sequential(\n",
       "                (0): Sequential(\n",
       "                  (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "                  (1): GELU(approximate='none')\n",
       "                )\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "                (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (2): Residual()\n",
       "          )\n",
       "          (16): ModuleList(\n",
       "            (0): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Attention(\n",
       "              (to_q): Linear(in_features=1280, out_features=512, bias=False)\n",
       "              (to_k): Linear(in_features=1280, out_features=512, bias=False)\n",
       "              (to_v): Linear(in_features=1280, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (to_out): Linear(in_features=512, out_features=1280, bias=True)\n",
       "            )\n",
       "            (2): Residual()\n",
       "          )\n",
       "          (17): ModuleList(\n",
       "            (0): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): FeedForward(\n",
       "              (net): Sequential(\n",
       "                (0): Sequential(\n",
       "                  (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "                  (1): GELU(approximate='none')\n",
       "                )\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "                (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (2): Residual()\n",
       "          )\n",
       "          (18): ModuleList(\n",
       "            (0): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Attention(\n",
       "              (to_q): Linear(in_features=1280, out_features=512, bias=False)\n",
       "              (to_k): Linear(in_features=1280, out_features=512, bias=False)\n",
       "              (to_v): Linear(in_features=1280, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (to_out): Linear(in_features=512, out_features=1280, bias=True)\n",
       "            )\n",
       "            (2): Residual()\n",
       "          )\n",
       "          (19): ModuleList(\n",
       "            (0): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): FeedForward(\n",
       "              (net): Sequential(\n",
       "                (0): Sequential(\n",
       "                  (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "                  (1): GELU(approximate='none')\n",
       "                )\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "                (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (2): Residual()\n",
       "          )\n",
       "          (20): ModuleList(\n",
       "            (0): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Attention(\n",
       "              (to_q): Linear(in_features=1280, out_features=512, bias=False)\n",
       "              (to_k): Linear(in_features=1280, out_features=512, bias=False)\n",
       "              (to_v): Linear(in_features=1280, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (to_out): Linear(in_features=512, out_features=1280, bias=True)\n",
       "            )\n",
       "            (2): Residual()\n",
       "          )\n",
       "          (21): ModuleList(\n",
       "            (0): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): FeedForward(\n",
       "              (net): Sequential(\n",
       "                (0): Sequential(\n",
       "                  (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "                  (1): GELU(approximate='none')\n",
       "                )\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "                (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (2): Residual()\n",
       "          )\n",
       "          (22): ModuleList(\n",
       "            (0): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Attention(\n",
       "              (to_q): Linear(in_features=1280, out_features=512, bias=False)\n",
       "              (to_k): Linear(in_features=1280, out_features=512, bias=False)\n",
       "              (to_v): Linear(in_features=1280, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (to_out): Linear(in_features=512, out_features=1280, bias=True)\n",
       "            )\n",
       "            (2): Residual()\n",
       "          )\n",
       "          (23): ModuleList(\n",
       "            (0): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): FeedForward(\n",
       "              (net): Sequential(\n",
       "                (0): Sequential(\n",
       "                  (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "                  (1): GELU(approximate='none')\n",
       "                )\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "                (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (2): Residual()\n",
       "          )\n",
       "          (24): ModuleList(\n",
       "            (0): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Attention(\n",
       "              (to_q): Linear(in_features=1280, out_features=512, bias=False)\n",
       "              (to_k): Linear(in_features=1280, out_features=512, bias=False)\n",
       "              (to_v): Linear(in_features=1280, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (to_out): Linear(in_features=512, out_features=1280, bias=True)\n",
       "            )\n",
       "            (2): Residual()\n",
       "          )\n",
       "          (25): ModuleList(\n",
       "            (0): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): FeedForward(\n",
       "              (net): Sequential(\n",
       "                (0): Sequential(\n",
       "                  (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "                  (1): GELU(approximate='none')\n",
       "                )\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "                (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (2): Residual()\n",
       "          )\n",
       "          (26): ModuleList(\n",
       "            (0): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Attention(\n",
       "              (to_q): Linear(in_features=1280, out_features=512, bias=False)\n",
       "              (to_k): Linear(in_features=1280, out_features=512, bias=False)\n",
       "              (to_v): Linear(in_features=1280, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (to_out): Linear(in_features=512, out_features=1280, bias=True)\n",
       "            )\n",
       "            (2): Residual()\n",
       "          )\n",
       "          (27): ModuleList(\n",
       "            (0): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): FeedForward(\n",
       "              (net): Sequential(\n",
       "                (0): Sequential(\n",
       "                  (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "                  (1): GELU(approximate='none')\n",
       "                )\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "                (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (2): Residual()\n",
       "          )\n",
       "          (28): ModuleList(\n",
       "            (0): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Attention(\n",
       "              (to_q): Linear(in_features=1280, out_features=512, bias=False)\n",
       "              (to_k): Linear(in_features=1280, out_features=512, bias=False)\n",
       "              (to_v): Linear(in_features=1280, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (to_out): Linear(in_features=512, out_features=1280, bias=True)\n",
       "            )\n",
       "            (2): Residual()\n",
       "          )\n",
       "          (29): ModuleList(\n",
       "            (0): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): FeedForward(\n",
       "              (net): Sequential(\n",
       "                (0): Sequential(\n",
       "                  (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "                  (1): GELU(approximate='none')\n",
       "                )\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "                (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (2): Residual()\n",
       "          )\n",
       "          (30): ModuleList(\n",
       "            (0): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Attention(\n",
       "              (to_q): Linear(in_features=1280, out_features=512, bias=False)\n",
       "              (to_k): Linear(in_features=1280, out_features=512, bias=False)\n",
       "              (to_v): Linear(in_features=1280, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (to_out): Linear(in_features=512, out_features=1280, bias=True)\n",
       "            )\n",
       "            (2): Residual()\n",
       "          )\n",
       "          (31): ModuleList(\n",
       "            (0): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): FeedForward(\n",
       "              (net): Sequential(\n",
       "                (0): Sequential(\n",
       "                  (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "                  (1): GELU(approximate='none')\n",
       "                )\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "                (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (2): Residual()\n",
       "          )\n",
       "          (32): ModuleList(\n",
       "            (0): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Attention(\n",
       "              (to_q): Linear(in_features=1280, out_features=512, bias=False)\n",
       "              (to_k): Linear(in_features=1280, out_features=512, bias=False)\n",
       "              (to_v): Linear(in_features=1280, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (to_out): Linear(in_features=512, out_features=1280, bias=True)\n",
       "            )\n",
       "            (2): Residual()\n",
       "          )\n",
       "          (33): ModuleList(\n",
       "            (0): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): FeedForward(\n",
       "              (net): Sequential(\n",
       "                (0): Sequential(\n",
       "                  (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "                  (1): GELU(approximate='none')\n",
       "                )\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "                (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (2): Residual()\n",
       "          )\n",
       "          (34): ModuleList(\n",
       "            (0): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Attention(\n",
       "              (to_q): Linear(in_features=1280, out_features=512, bias=False)\n",
       "              (to_k): Linear(in_features=1280, out_features=512, bias=False)\n",
       "              (to_v): Linear(in_features=1280, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (to_out): Linear(in_features=512, out_features=1280, bias=True)\n",
       "            )\n",
       "            (2): Residual()\n",
       "          )\n",
       "          (35): ModuleList(\n",
       "            (0): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): FeedForward(\n",
       "              (net): Sequential(\n",
       "                (0): Sequential(\n",
       "                  (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "                  (1): GELU(approximate='none')\n",
       "                )\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "                (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (2): Residual()\n",
       "          )\n",
       "          (36): ModuleList(\n",
       "            (0): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Attention(\n",
       "              (to_q): Linear(in_features=1280, out_features=512, bias=False)\n",
       "              (to_k): Linear(in_features=1280, out_features=512, bias=False)\n",
       "              (to_v): Linear(in_features=1280, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (to_out): Linear(in_features=512, out_features=1280, bias=True)\n",
       "            )\n",
       "            (2): Residual()\n",
       "          )\n",
       "          (37): ModuleList(\n",
       "            (0): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): FeedForward(\n",
       "              (net): Sequential(\n",
       "                (0): Sequential(\n",
       "                  (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "                  (1): GELU(approximate='none')\n",
       "                )\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "                (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (2): Residual()\n",
       "          )\n",
       "          (38): ModuleList(\n",
       "            (0): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Attention(\n",
       "              (to_q): Linear(in_features=1280, out_features=512, bias=False)\n",
       "              (to_k): Linear(in_features=1280, out_features=512, bias=False)\n",
       "              (to_v): Linear(in_features=1280, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (to_out): Linear(in_features=512, out_features=1280, bias=True)\n",
       "            )\n",
       "            (2): Residual()\n",
       "          )\n",
       "          (39): ModuleList(\n",
       "            (0): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): FeedForward(\n",
       "              (net): Sequential(\n",
       "                (0): Sequential(\n",
       "                  (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "                  (1): GELU(approximate='none')\n",
       "                )\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "                (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (2): Residual()\n",
       "          )\n",
       "          (40): ModuleList(\n",
       "            (0): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Attention(\n",
       "              (to_q): Linear(in_features=1280, out_features=512, bias=False)\n",
       "              (to_k): Linear(in_features=1280, out_features=512, bias=False)\n",
       "              (to_v): Linear(in_features=1280, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (to_out): Linear(in_features=512, out_features=1280, bias=True)\n",
       "            )\n",
       "            (2): Residual()\n",
       "          )\n",
       "          (41): ModuleList(\n",
       "            (0): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): FeedForward(\n",
       "              (net): Sequential(\n",
       "                (0): Sequential(\n",
       "                  (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "                  (1): GELU(approximate='none')\n",
       "                )\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "                (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (2): Residual()\n",
       "          )\n",
       "          (42): ModuleList(\n",
       "            (0): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Attention(\n",
       "              (to_q): Linear(in_features=1280, out_features=512, bias=False)\n",
       "              (to_k): Linear(in_features=1280, out_features=512, bias=False)\n",
       "              (to_v): Linear(in_features=1280, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (to_out): Linear(in_features=512, out_features=1280, bias=True)\n",
       "            )\n",
       "            (2): Residual()\n",
       "          )\n",
       "          (43): ModuleList(\n",
       "            (0): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): FeedForward(\n",
       "              (net): Sequential(\n",
       "                (0): Sequential(\n",
       "                  (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "                  (1): GELU(approximate='none')\n",
       "                )\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "                (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (2): Residual()\n",
       "          )\n",
       "          (44): ModuleList(\n",
       "            (0): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Attention(\n",
       "              (to_q): Linear(in_features=1280, out_features=512, bias=False)\n",
       "              (to_k): Linear(in_features=1280, out_features=512, bias=False)\n",
       "              (to_v): Linear(in_features=1280, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (to_out): Linear(in_features=512, out_features=1280, bias=True)\n",
       "            )\n",
       "            (2): Residual()\n",
       "          )\n",
       "          (45): ModuleList(\n",
       "            (0): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): FeedForward(\n",
       "              (net): Sequential(\n",
       "                (0): Sequential(\n",
       "                  (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "                  (1): GELU(approximate='none')\n",
       "                )\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "                (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (2): Residual()\n",
       "          )\n",
       "          (46): ModuleList(\n",
       "            (0): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Attention(\n",
       "              (to_q): Linear(in_features=1280, out_features=512, bias=False)\n",
       "              (to_k): Linear(in_features=1280, out_features=512, bias=False)\n",
       "              (to_v): Linear(in_features=1280, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (to_out): Linear(in_features=512, out_features=1280, bias=True)\n",
       "            )\n",
       "            (2): Residual()\n",
       "          )\n",
       "          (47): ModuleList(\n",
       "            (0): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): FeedForward(\n",
       "              (net): Sequential(\n",
       "                (0): Sequential(\n",
       "                  (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "                  (1): GELU(approximate='none')\n",
       "                )\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "                (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (2): Residual()\n",
       "          )\n",
       "          (48): ModuleList(\n",
       "            (0): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Attention(\n",
       "              (to_q): Linear(in_features=1280, out_features=512, bias=False)\n",
       "              (to_k): Linear(in_features=1280, out_features=512, bias=False)\n",
       "              (to_v): Linear(in_features=1280, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (to_out): Linear(in_features=512, out_features=1280, bias=True)\n",
       "            )\n",
       "            (2): Residual()\n",
       "          )\n",
       "          (49): ModuleList(\n",
       "            (0): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): FeedForward(\n",
       "              (net): Sequential(\n",
       "                (0): Sequential(\n",
       "                  (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "                  (1): GELU(approximate='none')\n",
       "                )\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "                (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (2): Residual()\n",
       "          )\n",
       "          (50): ModuleList(\n",
       "            (0): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Attention(\n",
       "              (to_q): Linear(in_features=1280, out_features=512, bias=False)\n",
       "              (to_k): Linear(in_features=1280, out_features=512, bias=False)\n",
       "              (to_v): Linear(in_features=1280, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (to_out): Linear(in_features=512, out_features=1280, bias=True)\n",
       "            )\n",
       "            (2): Residual()\n",
       "          )\n",
       "          (51): ModuleList(\n",
       "            (0): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): FeedForward(\n",
       "              (net): Sequential(\n",
       "                (0): Sequential(\n",
       "                  (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "                  (1): GELU(approximate='none')\n",
       "                )\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "                (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (2): Residual()\n",
       "          )\n",
       "          (52): ModuleList(\n",
       "            (0): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Attention(\n",
       "              (to_q): Linear(in_features=1280, out_features=512, bias=False)\n",
       "              (to_k): Linear(in_features=1280, out_features=512, bias=False)\n",
       "              (to_v): Linear(in_features=1280, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (to_out): Linear(in_features=512, out_features=1280, bias=True)\n",
       "            )\n",
       "            (2): Residual()\n",
       "          )\n",
       "          (53): ModuleList(\n",
       "            (0): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): FeedForward(\n",
       "              (net): Sequential(\n",
       "                (0): Sequential(\n",
       "                  (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "                  (1): GELU(approximate='none')\n",
       "                )\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "                (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (2): Residual()\n",
       "          )\n",
       "          (54): ModuleList(\n",
       "            (0): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Attention(\n",
       "              (to_q): Linear(in_features=1280, out_features=512, bias=False)\n",
       "              (to_k): Linear(in_features=1280, out_features=512, bias=False)\n",
       "              (to_v): Linear(in_features=1280, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (to_out): Linear(in_features=512, out_features=1280, bias=True)\n",
       "            )\n",
       "            (2): Residual()\n",
       "          )\n",
       "          (55): ModuleList(\n",
       "            (0): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): FeedForward(\n",
       "              (net): Sequential(\n",
       "                (0): Sequential(\n",
       "                  (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "                  (1): GELU(approximate='none')\n",
       "                )\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "                (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (2): Residual()\n",
       "          )\n",
       "          (56): ModuleList(\n",
       "            (0): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Attention(\n",
       "              (to_q): Linear(in_features=1280, out_features=512, bias=False)\n",
       "              (to_k): Linear(in_features=1280, out_features=512, bias=False)\n",
       "              (to_v): Linear(in_features=1280, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (to_out): Linear(in_features=512, out_features=1280, bias=True)\n",
       "            )\n",
       "            (2): Residual()\n",
       "          )\n",
       "          (57): ModuleList(\n",
       "            (0): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): FeedForward(\n",
       "              (net): Sequential(\n",
       "                (0): Sequential(\n",
       "                  (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "                  (1): GELU(approximate='none')\n",
       "                )\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "                (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (2): Residual()\n",
       "          )\n",
       "          (58): ModuleList(\n",
       "            (0): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Attention(\n",
       "              (to_q): Linear(in_features=1280, out_features=512, bias=False)\n",
       "              (to_k): Linear(in_features=1280, out_features=512, bias=False)\n",
       "              (to_v): Linear(in_features=1280, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (to_out): Linear(in_features=512, out_features=1280, bias=True)\n",
       "            )\n",
       "            (2): Residual()\n",
       "          )\n",
       "          (59): ModuleList(\n",
       "            (0): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): FeedForward(\n",
       "              (net): Sequential(\n",
       "                (0): Sequential(\n",
       "                  (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "                  (1): GELU(approximate='none')\n",
       "                )\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "                (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (2): Residual()\n",
       "          )\n",
       "          (60): ModuleList(\n",
       "            (0): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Attention(\n",
       "              (to_q): Linear(in_features=1280, out_features=512, bias=False)\n",
       "              (to_k): Linear(in_features=1280, out_features=512, bias=False)\n",
       "              (to_v): Linear(in_features=1280, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (to_out): Linear(in_features=512, out_features=1280, bias=True)\n",
       "            )\n",
       "            (2): Residual()\n",
       "          )\n",
       "          (61): ModuleList(\n",
       "            (0): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): FeedForward(\n",
       "              (net): Sequential(\n",
       "                (0): Sequential(\n",
       "                  (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "                  (1): GELU(approximate='none')\n",
       "                )\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "                (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (2): Residual()\n",
       "          )\n",
       "          (62): ModuleList(\n",
       "            (0): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Attention(\n",
       "              (to_q): Linear(in_features=1280, out_features=512, bias=False)\n",
       "              (to_k): Linear(in_features=1280, out_features=512, bias=False)\n",
       "              (to_v): Linear(in_features=1280, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (to_out): Linear(in_features=512, out_features=1280, bias=True)\n",
       "            )\n",
       "            (2): Residual()\n",
       "          )\n",
       "          (63): ModuleList(\n",
       "            (0): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): FeedForward(\n",
       "              (net): Sequential(\n",
       "                (0): Sequential(\n",
       "                  (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "                  (1): GELU(approximate='none')\n",
       "                )\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "                (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (2): Residual()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "      (to_logits): Linear(in_features=1280, out_features=30522, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 50/50 [00:09<00:00,  5.48it/s]\n"
     ]
    }
   ],
   "source": [
    "sampler = DDIMSamper(model)\n",
    "\n",
    "sample_path = Path(opt.outdir) / \"samples\"\n",
    "sample_path.mkdir(parents=True, exist_ok=True)\n",
    "base_count = len(list(sample_path.glob(\"*.png\")))\n",
    "\n",
    "with torch.no_grad():\n",
    "    uc = model.cond_stage_model.encode(opt.n_samples * [\"\"])\n",
    "    c = model.cond_stage_model.encode(opt.n_samples * [opt.prompt])\n",
    "    img_shape = [4, opt.H // 8, opt.W // 8]\n",
    "    samples_ddim = sampler.sample(\n",
    "        ddim_steps=opt.ddim_steps,\n",
    "        batch_size=opt.n_samples,\n",
    "        img_shape=img_shape,\n",
    "        conditioning=c,\n",
    "        unconditional_guidance_scale=opt.scale,\n",
    "        unconditional_conditioning=uc,\n",
    "    )\n",
    "    samples_ddim = 1.0 / model.scale_factor * samples_ddim\n",
    "    x_samples_ddim = model.first_stage_model.decode(samples_ddim)\n",
    "    x_samples_ddim = torch.clamp((x_samples_ddim + 1.0) / 2.0, min=0.0, max=1.0)\n",
    "\n",
    "    for x_sample in x_samples_ddim:\n",
    "        x_sample = 255.0 * rearrange(x_sample.cpu().numpy(), \"c h w -> h w c\")\n",
    "        Image.fromarray(x_sample.astype(np.uint8)).save(\n",
    "            sample_path / f\"{base_count:04}.png\"\n",
    "        )\n",
    "        base_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ldm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
