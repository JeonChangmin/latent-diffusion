{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/changminjeon/.local/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-03-08 04:38:58.178902: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-03-08 04:38:58.222795: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-08 04:38:58.967551: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from ldm.models.autoencoder import AutoencoderKL  # First Stage\n",
    "from ldm.modules.encoders.modules import BERTEmbedder  # Condition Model\n",
    "from ldm.modules.diffusionmodules.openaimodel import UNetModel  # Diffusion Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiffusionWrapper(nn.Module):\n",
    "    def __init__(self, diff_model_config):\n",
    "        super().__init__()\n",
    "        self.diffusion_model = UNetModel(**diff_model_config)\n",
    "\n",
    "    def forward(self, x, t, c_crossattn: list = None):\n",
    "        cc = torch.cat(c_crossattn, 1)\n",
    "        out = self.diffusion_model(x, t, context=cc)\n",
    "        return out\n",
    "\n",
    "\n",
    "class DDPM(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        unet_config,\n",
    "        timesteps=1000,\n",
    "        image_size=256,\n",
    "        channels=3,\n",
    "        log_every_t=100,\n",
    "        clip_denoised=True,\n",
    "        linear_start=1e-4,\n",
    "        linear_end=2e-2,\n",
    "        original_elbo_weight=0.0,\n",
    "        v_posterior=0.0,  # weight for choosing posterior variance as sigma = (1-v) * beta_tilde + v * beta\n",
    "        l_simple_weight=1.0,\n",
    "        use_positional_encodings=False,\n",
    "        learn_logvar=False,\n",
    "        logvar_init=0.0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.cond_stage_model = None\n",
    "        self.clip_denoised = clip_denoised\n",
    "        self.log_every_t = log_every_t\n",
    "        self.image_size = image_size  # try conv?\n",
    "        self.channels = channels\n",
    "        self.use_positional_encodings = use_positional_encodings\n",
    "        self.model = UNetModel(**unet_config)\n",
    "\n",
    "        self.v_posterior = v_posterior\n",
    "        self.original_elbo_weight = original_elbo_weight\n",
    "        self.l_simple_weight = l_simple_weight\n",
    "\n",
    "        #### register_schedule() start\n",
    "        betas = np.linspace(linear_start ** 0.5, linear_end ** 0.5, timesteps, dtype=np.float64) ** 2\n",
    "        alphas = 1. - betas\n",
    "        alphas_cumprod = np.cumprod(alphas, axis=0)\n",
    "        alphas_cumprod_prev = np.append(1., alphas_cumprod[:-1])\n",
    "\n",
    "        timesteps, = betas.shape\n",
    "        self.num_timesteps = int(timesteps)\n",
    "        self.linear_start = linear_start\n",
    "        self.linear_end = linear_end\n",
    "        assert alphas_cumprod.shape[0] == self.num_timesteps, 'alphas have to be defined for each timestep'\n",
    "\n",
    "        to_torch = partial(torch.tensor, dtype=torch.float32)\n",
    "\n",
    "        self.register_buffer('betas', to_torch(betas))\n",
    "        self.register_buffer('alphas_cumprod', to_torch(alphas_cumprod))\n",
    "        self.register_buffer('alphas_cumprod_prev', to_torch(alphas_cumprod_prev))\n",
    "\n",
    "        # calculations for diffusion q(x_t | x_{t-1}) and others\n",
    "        self.register_buffer('sqrt_alphas_cumprod', to_torch(np.sqrt(alphas_cumprod)))\n",
    "        self.register_buffer('sqrt_one_minus_alphas_cumprod', to_torch(np.sqrt(1. - alphas_cumprod)))\n",
    "        self.register_buffer('log_one_minus_alphas_cumprod', to_torch(np.log(1. - alphas_cumprod)))\n",
    "        self.register_buffer('sqrt_recip_alphas_cumprod', to_torch(np.sqrt(1. / alphas_cumprod)))\n",
    "        self.register_buffer('sqrt_recipm1_alphas_cumprod', to_torch(np.sqrt(1. / alphas_cumprod - 1)))\n",
    "\n",
    "        # calculations for posterior q(x_{t-1} | x_t, x_0)\n",
    "        posterior_variance = (1 - self.v_posterior) * betas * (1. - alphas_cumprod_prev) / (1. - alphas_cumprod) + self.v_posterior * betas\n",
    "        # above: equal to 1. / (1. / (1. - alpha_cumprod_tm1) + alpha_t / beta_t)\n",
    "        self.register_buffer('posterior_variance', to_torch(posterior_variance))\n",
    "        # below: log calculation clipped because the posterior variance is 0 at the beginning of the diffusion chain\n",
    "        self.register_buffer('posterior_log_variance_clipped', to_torch(np.log(np.maximum(posterior_variance, 1e-20))))\n",
    "        self.register_buffer('posterior_mean_coef1', to_torch(\n",
    "            betas * np.sqrt(alphas_cumprod_prev) / (1. - alphas_cumprod)))\n",
    "        self.register_buffer('posterior_mean_coef2', to_torch(\n",
    "            (1. - alphas_cumprod_prev) * np.sqrt(alphas) / (1. - alphas_cumprod)))\n",
    "\n",
    "        lvlb_weights = self.betas ** 2 / (2 * self.posterior_variance * to_torch(alphas) * (1 - self.alphas_cumprod))\n",
    "        # TODO how to choose this term\n",
    "        lvlb_weights[0] = lvlb_weights[1]\n",
    "        self.register_buffer('lvlb_weights', lvlb_weights, persistent=False)\n",
    "        assert not torch.isnan(self.lvlb_weights).all()\n",
    "        #### register_schedule() end\n",
    "\n",
    "        self.learn_logvar = learn_logvar\n",
    "        self.logvar = torch.full(fill_value=logvar_init, size=(self.num_timesteps,))\n",
    "\n",
    "\n",
    "\n",
    "class LatentDiffusion(DDPM):\n",
    "    def __init__(\n",
    "        self,\n",
    "        first_stage_config,\n",
    "        cond_stage_config,\n",
    "        num_timesteps_cond=None,\n",
    "        cond_stage_key=\"image\",\n",
    "        cond_stage_trainable=False,\n",
    "        concat_mode=True,\n",
    "        cond_stage_forward=None,\n",
    "        scale_factor=1.0,\n",
    "        *args,\n",
    "        **kwargs\n",
    "    ):\n",
    "        self.num_timesteps_cond = num_timesteps_cond\n",
    "        assert self.num_timesteps_cond <= kwargs[\"timesteps\"]\n",
    "        ckpt_path = kwargs.pop(\"ckpt_path\", None)\n",
    "        ignore_keys = kwargs.pop(\"ignore_keys\", [])\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.concat_mode = concat_mode\n",
    "        self.cond_stage_trainable = cond_stage_trainable\n",
    "        self.cond_stage_key = cond_stage_key\n",
    "        self.num_downs = len(first_stage_config[\"ddconfig\"][\"ch_mult\"]) - 1\n",
    "        self.scale_factor = scale_factor\n",
    "\n",
    "        self.first_stage_model = AutoencoderKL(**first_stage_config)\n",
    "        self.cond_stage_model = BERTEmbedder(**cond_stage_config)\n",
    "\n",
    "        self.cond_stage_forward = cond_stage_forward\n",
    "        self.clip_denoised = False\n",
    "        self.bbox_tokenizer = None  \n",
    "\n",
    "        self.restarted_from_ckpt = False\n",
    "\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def decode_first_stage(self, z):\n",
    "        z = 1. / self.scale_factor * z\n",
    "        return self.first_stage_model.decode(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDIMSamper:\n",
    "    def __init__(self, model: LatentDiffusion):\n",
    "        self.model = model\n",
    "        self.ddpm_num_time_steps = model.num_timesteps\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def sample(\n",
    "        self,\n",
    "        ddim_steps,\n",
    "        batch_size,\n",
    "        shape,\n",
    "        conditioning=None,\n",
    "        callback=None,\n",
    "        normals_sequence=None,\n",
    "        img_callback=None,\n",
    "        quantize_x0=False,\n",
    "        eta=0.0,\n",
    "        x0=None,\n",
    "        temperature=1.0,\n",
    "        noise_dropout=0.0,\n",
    "        score_corrector=None,\n",
    "        corrector_kwargs=None,\n",
    "        verbose=True,\n",
    "        x_T=None,\n",
    "        log_every_t=100,\n",
    "        unconditional_guidance_scale=1.0,\n",
    "        unconditional_conditioning=None,\n",
    "    ):\n",
    "        self.ddim_timesteps = np.arange(0, self.ddpm_num_timesteps, ddpm_num_timesteps // ddim_steps) + 1\n",
    "\n",
    "        # ddim sampling parameters\n",
    "        ddim_sigmas, ddim_alphas, ddim_alphas_prev = make_ddim_sampling_parameters(alphacums=alphas_cumprod.cpu(),\n",
    "                                                                                   ddim_timesteps=self.ddim_timesteps,\n",
    "                                                                                   eta=ddim_eta,verbose=verbose)\n",
    "        self.register_buffer('ddim_sigmas', ddim_sigmas)\n",
    "        self.register_buffer('ddim_alphas', ddim_alphas)\n",
    "        self.register_buffer('ddim_alphas_prev', ddim_alphas_prev)\n",
    "        self.register_buffer('ddim_sqrt_one_minus_alphas', np.sqrt(1. - ddim_alphas))\n",
    "        sigmas_for_original_sampling_steps = ddim_eta * torch.sqrt((1 - self.alphas_cumprod_prev) / (1 - self.alphas_cumprod) * (1 - self.alphas_cumprod / self.alphas_cumprod_prev))\n",
    "        self.register_buffer('ddim_sigmas_for_original_num_steps', sigmas_for_original_sampling_steps)\n",
    "\n",
    "        # sampling\n",
    "        C, H, W = shape\n",
    "        size = (batch_size, C, H, W)\n",
    "        print(f\"Data shape for DDIM sampling is {size}, eta {eta}\")\n",
    "\n",
    "        samples, intermediates = self.ddim_sampling(\n",
    "            conditioning,\n",
    "            size,\n",
    "            callback=callback,\n",
    "            img_callback=img_callback,\n",
    "            quantize_denoised=quantize_x0,\n",
    "            x0=x0,\n",
    "            noise_dropout=noise_dropout,\n",
    "            temperature=temperature,\n",
    "            score_corrector=score_corrector,\n",
    "            corrector_kwargs=corrector_kwargs,\n",
    "            x_T=x_T,\n",
    "            log_every_t=log_every_t,\n",
    "            unconditional_guidance_scale=unconditional_guidance_scale,\n",
    "            unconditional_conditioning=unconditional_conditioning,\n",
    "        )\n",
    "        return samples, intermediates\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def ddim_sampling(self, cond, shape,\n",
    "                      x_T=None,\n",
    "                      callback=None, quantize_denoised=False,\n",
    "                      x0=None, img_callback=None, log_every_t=100,\n",
    "                      temperature=1., noise_dropout=0., score_corrector=None, corrector_kwargs=None,\n",
    "                      unconditional_guidance_scale=1., unconditional_conditioning=None,):\n",
    "        device = self.model.betas.device\n",
    "        b = shape[0]\n",
    "        img = torch.randn(shape, device=device)\n",
    "        timesteps = self.ddpm_num_timesteps\n",
    "        intermediates = {'x_inter': [img], 'pred_x0': [img]}\n",
    "        time_range = reversed(range(0, timesteps))\n",
    "        total_steps = timesteps\n",
    "\n",
    "        iterator = tqdm(time_range, desc='DDIM Sampler', total=total_steps)\n",
    "\n",
    "        for i, step in enumerate(iterator):\n",
    "            index = total_steps - i - 1\n",
    "            ts = torch.full((b,), step, device=device, dtype=torch.long)\n",
    "\n",
    "            outs = self.p_sample_ddim(img, cond, ts, index=index,\n",
    "                                      quantize_denoised=quantize_denoised, temperature=temperature,\n",
    "                                      noise_dropout=noise_dropout, score_corrector=score_corrector,\n",
    "                                      corrector_kwargs=corrector_kwargs,\n",
    "                                      unconditional_guidance_scale=unconditional_guidance_scale,\n",
    "                                      unconditional_conditioning=unconditional_conditioning)\n",
    "            img, pred_x0 = outs\n",
    "\n",
    "            if index % log_every_t == 0 or index == total_steps - 1:\n",
    "                intermediates['x_inter'].append(img)\n",
    "                intermediates['pred_x0'].append(pred_x0)\n",
    "\n",
    "        return img, intermediates\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def p_sample_ddim(self, x, c, t, index, repeat_noise=False, quantize_denoised=False,\n",
    "                      temperature=1., noise_dropout=0., score_corrector=None, corrector_kwargs=None,\n",
    "                      unconditional_guidance_scale=1., unconditional_conditioning=None):\n",
    "        b, *_, device = *x.shape, x.device\n",
    "\n",
    "        if unconditional_conditioning is None or unconditional_guidance_scale == 1.:\n",
    "            e_t = self.model.apply_model(x, t, c)\n",
    "        else:\n",
    "            x_in = torch.cat([x] * 2)\n",
    "            t_in = torch.cat([t] * 2)\n",
    "            c_in = torch.cat([unconditional_conditioning, c])\n",
    "            e_t_uncond, e_t = self.model.apply_model(x_in, t_in, c_in).chunk(2)\n",
    "            e_t = e_t_uncond + unconditional_guidance_scale * (e_t - e_t_uncond)\n",
    "\n",
    "        alphas = self.model.alphas_cumprod\n",
    "        alphas_prev = self.model.alphas_cumprod_prev\n",
    "        sqrt_one_minus_alphas = self.model.sqrt_one_minus_alphas_cumprod\n",
    "        sigmas = self.model.ddim_sigmas_for_original_num_steps\n",
    "        # select parameters corresponding to the currently considered timestep\n",
    "        a_t = torch.full((b, 1, 1, 1), alphas[index], device=device)\n",
    "        a_prev = torch.full((b, 1, 1, 1), alphas_prev[index], device=device)\n",
    "        sigma_t = torch.full((b, 1, 1, 1), sigmas[index], device=device)\n",
    "        sqrt_one_minus_at = torch.full((b, 1, 1, 1), sqrt_one_minus_alphas[index],device=device)\n",
    "\n",
    "        # current prediction for x_0\n",
    "        pred_x0 = (x - sqrt_one_minus_at * e_t) / a_t.sqrt()\n",
    "        # direction pointing to x_t\n",
    "        dir_xt = (1. - a_prev - sigma_t**2).sqrt() * e_t\n",
    "        noise = sigma_t * noise_like(x.shape, device, repeat_noise) * temperature\n",
    "        x_prev = a_prev.sqrt() * pred_x0 + dir_xt + noise\n",
    "        return x_prev, pred_x0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(H=256, W=256, ddim_eta=0.0, ddim_steps=50, n_iter=4, n_samples=4, outdir='outputs/txt2img-samples', plms=False, prompt='a virus monster is playing guitar, oil on canvas', scale=5.0)\n"
     ]
    }
   ],
   "source": [
    "opt = Namespace()\n",
    "opt.prompt = \"a virus monster is playing guitar, oil on canvas\"\n",
    "opt.ddim_eta = 0.0\n",
    "opt.n_samples = 4\n",
    "opt.n_iter = 4\n",
    "opt.scale = 5.0\n",
    "opt.ddim_steps = 50\n",
    "opt.outdir = \"outputs/txt2img-samples\"\n",
    "opt.plms = False\n",
    "opt.H = 256\n",
    "opt.W = 256\n",
    "print(opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'channels': 4,\n",
      " 'cond_stage_config': {'n_embed': 1280, 'n_layer': 32},\n",
      " 'cond_stage_key': 'caption',\n",
      " 'cond_stage_trainable': True,\n",
      " 'conditioning_key': 'crossattn',\n",
      " 'first_stage_config': {'ddconfig': {'attn_resolutions': [],\n",
      "                                     'ch': 128,\n",
      "                                     'ch_mult': [1, 2, 4, 4],\n",
      "                                     'double_z': True,\n",
      "                                     'dropout': 0.0,\n",
      "                                     'in_channels': 3,\n",
      "                                     'num_res_blocks': 2,\n",
      "                                     'out_ch': 3,\n",
      "                                     'resolution': 256,\n",
      "                                     'z_channels': 4},\n",
      "                        'embed_dim': 4,\n",
      "                        'lossconfig': {'target': 'torch.nn.Identity'},\n",
      "                        'monitor': 'val/rec_loss'},\n",
      " 'first_stage_key': 'image',\n",
      " 'image_size': 32,\n",
      " 'linear_end': 0.012,\n",
      " 'linear_start': 0.00085,\n",
      " 'log_every_t': 200,\n",
      " 'monitor': 'val/loss_simple_ema',\n",
      " 'num_timesteps_cond': 1,\n",
      " 'scale_factor': 0.18215,\n",
      " 'timesteps': 1000,\n",
      " 'unet_config': {'attention_resolutions': [4, 2, 1],\n",
      "                 'channel_mult': [1, 2, 4, 4],\n",
      "                 'context_dim': 1280,\n",
      "                 'image_size': 32,\n",
      "                 'in_channels': 4,\n",
      "                 'legacy': False,\n",
      "                 'model_channels': 320,\n",
      "                 'num_heads': 8,\n",
      "                 'num_res_blocks': 2,\n",
      "                 'out_channels': 4,\n",
      "                 'transformer_depth': 1,\n",
      "                 'use_checkpoint': True,\n",
      "                 'use_spatial_transformer': True}}\n"
     ]
    }
   ],
   "source": [
    "# Load yaml configs\n",
    "with Path(\"latent-diffusion.yaml\").open(\"r\") as f:\n",
    "    model_config = yaml.load(f, Loader=yaml.FullLoader)\n",
    "    pprint(model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = torch.load(\"models/ldm/text2img-large/model.ckpt\", map_location=\"cpu\")[\"state_dict\"]\n",
    "print([k for k in state_dict.keys() if \"alphas_cumprod\" in k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LatentDiffusion(**model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_path = os.path.join(outpath, \"samples\")\n",
    "os.makedirs(sample_path, exist_ok=True)\n",
    "base_count = len(os.listdir(sample_path))\n",
    "\n",
    "with torch.no_grad():\n",
    "    uc = model.cond_stage_model.encode(opt.n_samples * [\"\"])\n",
    "    c = model.cond_stage_model.encode(opt.n_samples * [prompt])\n",
    "    shape = [4, opt.H//8, opt.W//8]\n",
    "    samples_ddim, _ = sampler.sample(ddim_steps=opt.ddim_steps,\n",
    "                                    conditioning=c,\n",
    "                                    batch_size=opt.n_samples,\n",
    "                                    shape=shape,\n",
    "                                    verbose=False,\n",
    "                                    unconditional_guidance_scale=opt.scale,\n",
    "                                    unconditional_conditioning=uc,\n",
    "                                    eta=opt.ddim_eta)\n",
    "\n",
    "    samples_ddim = 1. / self.scale_factor * samples_ddim5\n",
    "    x_samples_ddim = model.first_stage_model.decode(samples_ddim)\n",
    "    x_samples_ddim = torch.clamp((x_samples_ddim+1.0)/2.0, min=0.0, max=1.0)\n",
    "\n",
    "    for x_sample in x_samples_ddim:\n",
    "        x_sample = 255. * rearrange(x_sample.cpu().numpy(), 'c h w -> h w c')\n",
    "        Image.fromarray(x_sample.astype(np.uint8)).save(os.path.join(sample_path, f\"{base_count:04}.png\"))\n",
    "        base_count += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ldm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
