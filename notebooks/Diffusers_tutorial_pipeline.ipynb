{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fSLDByfDmMu0"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "esCgWQCeQrke"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import torch\n",
        "from diffusers import DDPMPipeline\n",
        "from diffusers import DDPMScheduler, UNet2DModel\n",
        "from diffusers import DiffusionPipeline\n",
        "from diffusers import StableDiffusionPipeline\n",
        "from diffusers import (\n",
        "    DDPMScheduler,\n",
        "    DDIMScheduler,\n",
        "    PNDMScheduler,\n",
        "    LMSDiscreteScheduler,\n",
        "    EulerDiscreteScheduler,\n",
        "    EulerAncestralDiscreteScheduler,\n",
        "    DPMSolverMultistepScheduler,\n",
        ")\n",
        "device = torch.device('cuda')\n",
        "cache_dir = \"/data2/diffusion\"\n",
        "\n",
        "# ddpm\n",
        "ddpm = DDPMPipeline.from_pretrained(\"google/ddpm-cat-256\", cache_dir=cache_dir).to(device)\n",
        "scheduler = DDPMScheduler.from_pretrained(\"google/ddpm-cat-256\", cache_dir=cache_dir)\n",
        "model = UNet2DModel.from_pretrained(\"google/ddpm-cat-256\", cache_dir=cache_dir).to(device)\n",
        "\n",
        "# stablediffusion\n",
        "repo_id = \"runwayml/stable-diffusion-v1-5\"\n",
        "pipeline = DiffusionPipeline.from_pretrained(repo_id, cache_dir=cache_dir)\n",
        "\n",
        "# schedulers\n",
        "ddpm = DDPMScheduler.from_pretrained(repo_id, subfolder=\"scheduler\", cache_dir=cache_dir)\n",
        "ddim = DDIMScheduler.from_pretrained(repo_id, subfolder=\"scheduler\", cache_dir=cache_dir)\n",
        "pndm = PNDMScheduler.from_pretrained(repo_id, subfolder=\"scheduler\", cache_dir=cache_dir)\n",
        "lms = LMSDiscreteScheduler.from_pretrained(repo_id, subfolder=\"scheduler\", cache_dir=cache_dir)\n",
        "euler_anc = EulerAncestralDiscreteScheduler.from_pretrained(repo_id, subfolder=\"scheduler\", cache_dir=cache_dir)\n",
        "euler = EulerDiscreteScheduler.from_pretrained(repo_id, subfolder=\"scheduler\", cache_dir=cache_dir)\n",
        "dpm = DPMSolverMultistepScheduler.from_pretrained(repo_id, subfolder=\"scheduler\", cache_dir=cache_dir)\n",
        "\n",
        "# pipelines\n",
        "pipeline = StableDiffusionPipeline.from_pretrained(repo_id, scheduler=dpm, cache_dir=cache_dir).to(device)\n",
        "pipeline = StableDiffusionPipeline.from_pretrained(repo_id, torch_dtype=torch.bfloat16, cache_dir=cache_dir).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Diffusers\n",
        "What if we want to use and modify the diffusion model? It is difficult to use diffusion models directly from the source code.\n",
        "\n",
        "*Diffusers* enables easy use of diffusion models with minimal code\n",
        "\n",
        "The key words are:\n",
        "- **Library** for state-of-the art pretrained diffusion models\n",
        "- **Generate** images, audio, and 3D structures\n",
        "- **Modular** toolbox supporting simple inference and training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fiq7UibG6Rxp"
      },
      "source": [
        "# DiffusionPipeline\n",
        "DiffusionPipeline wraps the complexity of the entire diffusion system into an easy-to-use API, while remaining flexible enough to be adapted for other use cases, such as loading each component individually as building blocks to assemble your own diffusion system.\n",
        "\n",
        "This section will show you how to:\n",
        "\n",
        "- load *pipeline*s from the Hub\n",
        "- write our own denoising process\n",
        "- convert different components in the *pipeline*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "crwVQ_tP0SQ9"
      },
      "source": [
        "## What is *pipeline*?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dE2pLNOg0SQ9"
      },
      "source": [
        "\n",
        "DiffusionPipeline provides a quick and easy way to run a model for inference, requiring no more than four lines of code to generate an image:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8bHOU9LQ0SQ-"
      },
      "outputs": [],
      "source": [
        "from diffusers import DDPMPipeline\n",
        "\n",
        "ddpm = DDPMPipeline.from_pretrained(\"google/ddpm-cat-256\", cache_dir=cache_dir).to(device)\n",
        "image = ddpm(num_inference_steps=100).images[0]\n",
        "image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I9TVV0Tq0SQ_"
      },
      "source": [
        "<!-- <div class=\"flex justify-center\">\n",
        "    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/ddpm-cat.png\" alt=\"Image of cat created from DDPMPipeline\"/>\n",
        "</div> -->\n",
        "\n",
        "That was super easy, but how did the pipeline do that? Let's breakdown the pipeline and take a look at what's happening under the hood."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jyb-9CXm0Vl_"
      },
      "outputs": [],
      "source": [
        "print(ddpm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5VfS944e0Vl_"
      },
      "source": [
        "In the example above, the pipeline contains a [UNet2DModel](https://huggingface.co/docs/diffusers/main/en/api/models/unet2d#diffusers.UNet2DModel) model and a [DDPMScheduler](https://huggingface.co/docs/diffusers/main/en/api/schedulers/ddpm#diffusers.DDPMScheduler).\n",
        "\n",
        "Different models and schedulers can be used together.\n",
        "*   Models contains weights. (e.g., Transformer2D, ControlNet)\n",
        "*   Schedulers define how the denoising process is done. (e.g.,DDPM, DDIM)\n",
        "\n",
        "<div class=\"flex justify-center\">\n",
        "    <img src=\"https://lh7-us.googleusercontent.com/A6VP2MAvy0H6cxFDQ8d8InjdYQ3WDdWM2tmEgc8FycGOyURU2a5Wzr0kuRwi0sxTl0nS5K9MQ4Frpqv68dQO3G5NeQmRKzEfKMbzb_vxFQEXjOJF6AHcnN7gpCvm8fYZhEvki3lzHiqrY7ieygXyWt8\" width=\"400\" />\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9pRazhlrSSfF"
      },
      "source": [
        "The pipeline denoises an image by taking random noise the size of the desired output and passing it through the model several times. At each timestep, the model predicts the *noise residual* and the scheduler uses it to predict a less noisy image. The pipeline repeats this process until it reaches the end of the specified number of inference steps.\n",
        "\n",
        "To recreate the above denoising process without the pipeline, you can declare the model and scheduler separately, as shown below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n676gx0-SXRU"
      },
      "outputs": [],
      "source": [
        "from diffusers import DDPMScheduler, UNet2DModel\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "# Load the model and scheduler\n",
        "scheduler = DDPMScheduler.from_pretrained(\"google/ddpm-cat-256\", cache_dir=cache_dir)\n",
        "model = UNet2DModel.from_pretrained(\"google/ddpm-cat-256\", cache_dir=cache_dir).to(device)\n",
        "\n",
        "\n",
        "# Create some random noise with the same shape as the desired output\n",
        "sample_size = model.config.sample_size\n",
        "noise = torch.randn((1, 3, sample_size, sample_size)).to(device)\n",
        "\n",
        "# Write a loop to iterate over the timesteps.\n",
        "# At each timestep, the model does a UNet2DModel.forward() pass and returns the noisy residual.\n",
        "# The scheduler's step() method takes the noisy residual, timestep, and input and it predicts the image at the previous timestep.\n",
        "# This output becomes the next input to the model\n",
        "input = noise\n",
        "\n",
        "for t in scheduler.timesteps:\n",
        "    with torch.no_grad():\n",
        "        noisy_residual = model(input, t).sample\n",
        "    previous_noisy_sample = scheduler.step(noisy_residual, t, input).prev_sample\n",
        "    input = previous_noisy_sample\n",
        "\n",
        "# convert the denoised output into an image\n",
        "image = (input / 2 + 0.5).clamp(0, 1)\n",
        "image = image.cpu().permute(0, 2, 3, 1).numpy()[0]\n",
        "image = Image.fromarray((image * 255).round().astype(\"uint8\"))\n",
        "image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sRx7e9k7Penf"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "As a class method, [DiffusionPipeline.from_pretrained()](https://huggingface.co/docs/diffusers/main/en/api/pipelines/overview#diffusers.DiffusionPipeline.from_pretrained) is responsible for two things:\n",
        "\n",
        "- Download the latest version of the folder structure required for inference and cache it. If the latest folder structure is available in the local cache, [DiffusionPipeline.from_pretrained()](https://huggingface.co/docs/diffusers/main/en/api/pipelines/overview#diffusers.DiffusionPipeline.from_pretrained) reuses the cache and won't redownload the files.\n",
        "- Load the cached weights into the correct pipeline [class](https://huggingface.co/docs/diffusers/main/en/using-diffusers/./api/pipelines/overview#diffusers-summary) - retrieved from the `model_index.json` file - and return an instance of it.\n",
        "\n",
        "The pipelines underlying folder structure corresponds directly with their class instances. For example, the [StableDiffusionPipeline](https://huggingface.co/docs/diffusers/main/en/api/pipelines/stable_diffusion/text2img#diffusers.StableDiffusionPipeline) corresponds to the folder structure in [`runwayml/stable-diffusion-v1-5`](https://huggingface.co/runwayml/stable-diffusion-v1-5)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sPZde8J4Pmc4"
      },
      "outputs": [],
      "source": [
        "from diffusers import DiffusionPipeline\n",
        "\n",
        "repo_id = \"runwayml/stable-diffusion-v1-5\"\n",
        "pipeline = DiffusionPipeline.from_pretrained(repo_id, cache_dir=cache_dir)\n",
        "print(pipeline)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HP6zT4NG0Vl_"
      },
      "source": [
        "The above pipeline is an instance of [StableDiffusionPipeline](https://huggingface.co/docs/diffusers/main/en/api/pipelines/stable_diffusion/text2img#diffusers.StableDiffusionPipeline), which consists of seven components:\n",
        "\n",
        "- `\"feature_extractor\"`: a [CLIPFeatureExtractor](https://huggingface.co/docs/transformers/main/en/model_doc/clip#transformers.CLIPFeatureExtractor) from ðŸ¤— Transformers.\n",
        "- `\"safety_checker\"`: a [component](https://github.com/huggingface/diffusers/blob/e55687e1e15407f60f32242027b7bb8170e58266/src/diffusers/pipelines/stable_diffusion/safety_checker.py#L32) for screening against harmful content.\n",
        "- `\"scheduler\"`: an instance of [PNDMScheduler](https://huggingface.co/docs/diffusers/main/en/api/schedulers/pndm#diffusers.PNDMScheduler).\n",
        "- `\"text_encoder\"`: a [CLIPTextModel](https://huggingface.co/docs/transformers/main/en/model_doc/clip#transformers.CLIPTextModel) from ðŸ¤— Transformers.\n",
        "- `\"tokenizer\"`: a [CLIPTokenizer](https://huggingface.co/docs/transformers/main/en/model_doc/clip#transformers.CLIPTokenizer) from ðŸ¤— Transformers.\n",
        "- `\"unet\"`: an instance of [UNet2DConditionModel](https://huggingface.co/docs/diffusers/main/en/api/models/unet2d-cond#diffusers.UNet2DConditionModel).\n",
        "- `\"vae\"` an instance of [AutoencoderKL](https://huggingface.co/docs/diffusers/main/en/api/models/autoencoderkl#diffusers.AutoencoderKL).\n",
        "\n",
        "\n",
        "<div class=\"flex justify-center\">\n",
        "    <img src=\"https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/stable_diffusion.png\" width=\"400\" />\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mrU8gkyN9Ele"
      },
      "source": [
        "Compare the components of the pipeline instance to the [`runwayml/stable-diffusion-v1-5`](https://huggingface.co/runwayml/stable-diffusion-v1-5) folder structure, and you'll see there is a separate folder for each of the components in the repository:\n",
        "\n",
        "```\n",
        ".\n",
        "â”œâ”€â”€ feature_extractor\n",
        "â”‚Â Â  â””â”€â”€ preprocessor_config.json\n",
        "â”œâ”€â”€ model_index.json\n",
        "â”œâ”€â”€ safety_checker\n",
        "â”‚Â Â  â”œâ”€â”€ config.json\n",
        "â”‚Â Â  â””â”€â”€ pytorch_model.bin\n",
        "â”œâ”€â”€ scheduler\n",
        "â”‚Â Â  â””â”€â”€ scheduler_config.json\n",
        "â”œâ”€â”€ text_encoder\n",
        "â”‚Â Â  â”œâ”€â”€ config.json\n",
        "â”‚Â Â  â””â”€â”€ pytorch_model.bin\n",
        "â”œâ”€â”€ tokenizer\n",
        "â”‚Â Â  â”œâ”€â”€ merges.txt\n",
        "â”‚Â Â  â”œâ”€â”€ special_tokens_map.json\n",
        "â”‚Â Â  â”œâ”€â”€ tokenizer_config.json\n",
        "â”‚Â Â  â””â”€â”€ vocab.json\n",
        "â”œâ”€â”€ unet\n",
        "â”‚Â Â  â”œâ”€â”€ config.json\n",
        "â”‚Â Â  â”œâ”€â”€ diffusion_pytorch_model.bin\n",
        "â””â”€â”€ vae\n",
        "    â”œâ”€â”€ config.json\n",
        "    â”œâ”€â”€ diffusion_pytorch_model.bin\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zT3lDFBy8UW0"
      },
      "source": [
        "## Design a custom pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p4IvhT7E8wHm"
      },
      "source": [
        "### Schedulers\n",
        "\n",
        "Diffusion pipelines are inherently a collection of diffusion models and schedulers that are partly independent from each other. This means that one is able to switch out parts of the pipeline to better customize a pipeline to one's use case. The best example of this is the [Schedulers](https://huggingface.co/docs/diffusers/main/en/using-diffusers/../api/schedulers/overview.mdx).\n",
        "\n",
        "Whereas diffusion models usually simply define the forward pass from noise to a less noisy sample,\n",
        "schedulers define the whole denoising process, *i.e.*:\n",
        "- How many denoising steps?\n",
        "- Stochastic or deterministic?\n",
        "- What algorithm to use to find the denoised sample\n",
        "\n",
        "They can be quite complex and often define a trade-off between **denoising speed** and **denoising quality**.\n",
        "It is extremely difficult to measure quantitatively which scheduler works best for a given diffusion pipeline, so it is often recommended to simply try out which works best."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Szt9rRQsldPv"
      },
      "source": [
        "```python\n",
        "from diffusers import StableDiffusionPipeline\n",
        "from diffusers import DPMSolverMultistepScheduler\n",
        "\n",
        "# replace the pipelines scheduler with `DPMSolverMultistepScheduler`\n",
        "repo_id = \"runwayml/stable-diffusion-v1-5\"\n",
        "dpm = DPMSolverMultistepScheduler.from_pretrained(repo_id, subfolder=\"scheduler\")\n",
        "pipeline.scheduler = dpm\n",
        "\n",
        "# You can take a look at all available, compatible schedulers for the pipeline as follows\n",
        "pipeline.scheduler.compatibles\n",
        "\n",
        "# You can set the number of timesteps to run the denoising process\n",
        "image = pipeline(prompt, generator=generator, num_inference_steps=20).images[0]\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tzj8H5wxzMNE"
      },
      "source": [
        "### Accelerate inference\n",
        "\n",
        "With PyTorch 2 alone, you can accelerate the inference latency of text-to-image diffusion pipelines by up to 3x. This section will show you how to progressively apply the optimizations found in PyTorch 2 to reduce inference latency."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32tEYEzKzX5t"
      },
      "source": [
        "**Reduced precision**\n",
        "\n",
        "Enable the first optimization, reduced precision or more specifically bfloat16. There are several benefits of using reduced precision:\n",
        "\n",
        "\n",
        "\n",
        "*   Using a reduced numerical precision (such as float16 or bfloat16) for inference doesnâ€™t affect the generation quality but significantly improves latency.\n",
        "*   The benefits of using bfloat16 compared to float16 are hardware dependent, but modern GPUs tend to favor bfloat16.\n",
        "*   bfloat16 is much more resilient when used with quantization compared to float16, but more recent versions of the quantization library (torchao) we used donâ€™t have numerical issues with float16.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34Klgc90lYp9"
      },
      "source": [
        "```python\n",
        "pipeline = StableDiffusionPipeline.from_pretrained(\n",
        "    \"runwayml/stable-diffusion-v1-5\", torch_dtype=torch.bfloat16\n",
        ").to(device)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KQ4Yztcwz8oq"
      },
      "source": [
        "**Combine the attention blockâ€™s projection matrices**\n",
        "\n",
        "The UNet and VAE in SDXL use Transformer-like blocks which consists of attention blocks and feed-forward blocks.\n",
        "\n",
        "In an attention block, the input is projected into three sub-spaces using three different projection matrices â€“ Q, K, and V. These projections are performed separately on the input. But we can horizontally combine the projection matrices into a single matrix and perform the projection in one step. This increases the size of the matrix multiplications of the input projections and improves the impact of quantization.\n",
        "\n",
        "You can combine the projection matrices with just a single line of code:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NVZEbBNRlUbi"
      },
      "source": [
        "```python\n",
        "pipe.fuse_qkv_projections()\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AkAVXoHATEdT"
      },
      "source": [
        "### Let's try it out!\n",
        "Let's customize the following pipeline.\n",
        "\n",
        "Conditions:\n",
        "*   Do not change the prompt\n",
        "*   You can also modify components of the pipeline other than the scheduler.\n",
        "*   Must be an instance of StableDiffusionPipeline.\n",
        "\n",
        "Objectives:\n",
        "*   Design a fast pipeline.\n",
        "*   Design a high-quality pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OtV3Z19jTEdU"
      },
      "outputs": [],
      "source": [
        "prompt = \"A photograph of an astronaut riding a horse on Mars, high resolution, high definition.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v_gTfZMVIjy0"
      },
      "outputs": [],
      "source": [
        "from diffusers import StableDiffusionPipeline\n",
        "\n",
        "## example pipeline\n",
        "pipeline = StableDiffusionPipeline.from_pretrained(\"runwayml/stable-diffusion-v1-5\", cache_dir=cache_dir).to(device)\n",
        "generator = torch.Generator(device=device).manual_seed(8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cx_yV9dUIlgs"
      },
      "outputs": [],
      "source": [
        "image = pipeline(prompt, generator=generator).images[0]\n",
        "image"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "tzj8H5wxzMNE"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
