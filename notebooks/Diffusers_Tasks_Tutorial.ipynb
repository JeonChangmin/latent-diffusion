{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "398f2614",
   "metadata": {},
   "source": [
    "# Diffusers Task\n",
    "\n",
    "Let's setup the environment and download the models first.\n",
    "\n",
    "The [DiffusionPipeline](https://huggingface.co/docs/diffusers/main/en/api/pipelines/overview#diffusers.DiffusionPipeline) downloads and caches all modeling, tokenization, and scheduling components.\n",
    "Because the model consists of roughly 1.4 billion parameters, we strongly recommend running it on a GPU.\n",
    "\n",
    "Start by creating an instance of [DiffusionPipeline](https://huggingface.co/docs/diffusers/main/en/api/pipelines/overview#diffusers.DiffusionPipeline) and specify which pipeline checkpoint you would like to download.\n",
    "\n",
    "<!---\n",
    "I2I model replaced for faster download\n",
    "i2i_pipe = StableDiffusionImg2ImgPipeline.from_pretrained(\"nitrosocke/Ghibli-Diffusion\", torch_dtype=torch.float16)\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be963141",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from diffusers import (\n",
    "    DiffusionPipeline,\n",
    "    StableDiffusionImg2ImgPipeline,\n",
    "    StableDiffusionDepth2ImgPipeline,\n",
    ")\n",
    "\n",
    "cache_dir = \"/data2/diffusion\"\n",
    "\n",
    "# Unconditional Image Generation\n",
    "uncond_generator = DiffusionPipeline.from_pretrained(\n",
    "    \"anton-l/ddpm-butterflies-128\", cache_dir=cache_dir\n",
    ")\n",
    "\n",
    "# Conditional Image Generation\n",
    "cond_generator = DiffusionPipeline.from_pretrained(\n",
    "    \"runwayml/stable-diffusion-v1-5\", cache_dir=cache_dir\n",
    ")\n",
    "\n",
    "# Image to Image Generation\n",
    "i2i_pipe = StableDiffusionImg2ImgPipeline.from_pretrained(\n",
    "    \"runwayml/stable-diffusion-v1-5\",\n",
    "    torch_dtype=torch.float16,\n",
    "    cache_dir=cache_dir,\n",
    ")\n",
    "\n",
    "# Depth-conditioned Image Generation\n",
    "depth_pipe = StableDiffusionDepth2ImgPipeline.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-2-depth\",\n",
    "    torch_dtype=torch.float16,\n",
    "    cache_dir=cache_dir,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7017becf",
   "metadata": {},
   "source": [
    "## Stable Diffusion\n",
    "![The Stable Diffusion architecture](https://scholar.harvard.edu/sites/scholar.harvard.edu/files/styles/os_files_xxlarge/public/binxuw/files/stablediffusion_overview.jpg?m=1708096154&itok=n2gM0Xba)\n",
    "\n",
    "Stable Diffusion is the most popular open source foundation models for image generation. The details of the architecture is explained as a â€œLatent Diffusion Model\" in a previous session. The Stable Diffusion model uses the CFG (Classifier-free Guidance) which is highly related to parameters for the image generation.\n",
    "\n",
    "SDXL and SD3 inherits the similar architecture with improvement on prompt alignment and image quality. The Stable Diffusion 3 is the latest version yet to be released with [waitlist](https://stability.ai/stablediffusion3) available.\n",
    "\n",
    "In this tutorial, we will use Stable Diffusion 1.5, a finetuned version of the Stable Diffusion."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcdc0e1d",
   "metadata": {},
   "source": [
    "## Unconditional Image Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6af3047",
   "metadata": {},
   "source": [
    "Unconditional image generation is a relatively straightforward task. The model only generates images - without any additional context like text or an image - resembling the training data it was trained on. For this task, we will use a model trained to generate specific type of image.\n",
    "\n",
    "\n",
    "\n",
    "In this guide, you'll use [DiffusionPipeline](https://huggingface.co/docs/diffusers/main/en/api/pipelines/overview#diffusers.DiffusionPipeline) for unconditional image generation with [DDPM](https://arxiv.org/abs/2006.11239) (the checkpoint you'll use generates images of butterflies).\n",
    "\n",
    "You can use any of the ðŸ§¨ Diffusers [checkpoints](https://huggingface.co/models?library=diffusers&sort=downloads) from the Hub. If you want to use a different model, replace the \"anton-l/ddpm-butterflies-128\" with the model name to download and use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4213e3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncond_generator = DiffusionPipeline.from_pretrained(\"anton-l/ddpm-butterflies-128\", cache_dir=cache_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea04f1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = uncond_generator.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c76afd4",
   "metadata": {},
   "source": [
    "Now you can use the `generator` to generate an image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9744d7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = generator().images[0]\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74daef7",
   "metadata": {
    "id": "1-t-KfixAm5C"
   },
   "source": [
    "## Conditional image generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da98e329",
   "metadata": {
    "id": "cVxTGwZ2Am5E"
   },
   "source": [
    "Conditional image generation allows you to generate images from a text prompt. The text is converted into embeddings which are used to condition the model to generate an image from noise.\n",
    "\n",
    "The texts are tokenized and then CLIP model encodes texts. The cross-attention is used to guide image generation with text.\n",
    "\n",
    "The [DiffusionPipeline](https://huggingface.co/docs/diffusers/main/en/api/pipelines/overview#diffusers.DiffusionPipeline) is the easiest way to use a pre-trained diffusion system for inference.\n",
    "\n",
    "Start by creating an instance of [DiffusionPipeline](https://huggingface.co/docs/diffusers/main/en/api/pipelines/overview#diffusers.DiffusionPipeline) and specify which pipeline [checkpoint](https://huggingface.co/models?library=diffusers&sort=downloads) you would like to download.\n",
    "\n",
    "In this guide, you'll use [DiffusionPipeline](https://huggingface.co/docs/diffusers/main/en/api/pipelines/overview#diffusers.DiffusionPipeline) for text-to-image generation with [`runwayml/stable-diffusion-v1-5`](https://huggingface.co/runwayml/stable-diffusion-v1-5):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15afcfeb",
   "metadata": {
    "id": "eT0uJ8-NAm5G"
   },
   "outputs": [],
   "source": [
    "generator = cond_generator.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b443ec6",
   "metadata": {
    "id": "KoSi0ZXuAm5G"
   },
   "source": [
    "Now you can use the `generator` on your text prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369a5c30",
   "metadata": {
    "id": "hTB3xPUjAm5G"
   },
   "outputs": [],
   "source": [
    "image = generator(\"An image of a squirrel in Picasso style\").images[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf5a1dd",
   "metadata": {
    "id": "oKXM3_lQAm5H"
   },
   "source": [
    "The output is by default wrapped into a [`PIL.Image`](https://pillow.readthedocs.io/en/stable/reference/Image.html?highlight=image#the-image-class) object.\n",
    "\n",
    "You can save the image by calling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aee1cda",
   "metadata": {
    "id": "uKYVJZSEAm5H"
   },
   "outputs": [],
   "source": [
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be492c3",
   "metadata": {
    "id": "chOLmv9FAp2N"
   },
   "source": [
    "## Text-guided image-to-image generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21cd84a3",
   "metadata": {
    "id": "-ipUygi8Ap2O"
   },
   "source": [
    "The [StableDiffusionImg2ImgPipeline](https://huggingface.co/docs/diffusers/main/en/api/pipelines/stable_diffusion/img2img#diffusers.StableDiffusionImg2ImgPipeline) lets you pass a text prompt and an initial image to condition the generation of new images.\n",
    "\n",
    "In noise is added to the input image gradually as in the forward diffusion process. The image is then encoded to be used as a condition through cross-attention, like text conditions.\n",
    "\n",
    "Let's load the model to GPU first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b73c90",
   "metadata": {
    "id": "M5fQNDILAp2Q"
   },
   "outputs": [],
   "source": [
    "device = \"cuda\"\n",
    "pipe = i2i_pipe.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47648792",
   "metadata": {
    "id": "W7oTYqPxAp2Q"
   },
   "source": [
    "Download and preprocess an initial image so you can pass it to the pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd696b1b",
   "metadata": {
    "id": "Ru8tBDZGAp2Q"
   },
   "outputs": [],
   "source": [
    "url = \"https://raw.githubusercontent.com/CompVis/stable-diffusion/main/assets/stable-samples/img2img/sketch-mountains-input.jpg\"\n",
    "\n",
    "response = requests.get(url)\n",
    "init_image = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
    "init_image.thumbnail((768, 768))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c9c397a",
   "metadata": {
    "id": "y3RQtk7VAp2R"
   },
   "source": [
    "<Tip>\n",
    "\n",
    "ðŸ’¡ `strength` is a value between 0.0 and 1.0 that controls the amount of noise added to the input image. Values that approach 1.0 allow for lots of variations but will also produce images that are not semantically consistent with the input. The strength determines the number of steps for forward diffusion process on the conditioning image.\n",
    "\n",
    "</Tip>\n",
    "\n",
    "<Tip>\n",
    "\n",
    "ðŸ’¡ `guidance_scale` determines the scale of the conditioned inference of the CFG model. CFG infernece result is the weighted sum of conditional inference and unconditional inference. This parameter controls the weight of the conditional inference. The higher value leads to better alignment to the prompt and other conditions. \n",
    "    \n",
    "</Tip>\n",
    "\n",
    "Define the prompt (for this checkpoint finetuned on Ghibli-style art, you need to prefix the prompt with the `ghibli style` tokens) and run the pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b11f515",
   "metadata": {
    "id": "dqaEpZq6Ap2R"
   },
   "outputs": [],
   "source": [
    "prompt = \"ghibli style, a fantasy landscape with castles\"\n",
    "generator = torch.Generator(device=device).manual_seed(1024)\n",
    "image = pipe(prompt=prompt, image=init_image, strength=0.75, guidance_scale=7.5, generator=generator).images[0]\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895d516e",
   "metadata": {
    "id": "eY-u1mLtAp2S"
   },
   "source": [
    "### Sample Result\n",
    "| Input                                                                           | Output                                                                                |\n",
    "|---------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| <img src=\"https://huggingface.co/datasets/YiYiXu/test-doc-assets/resolve/main/image_2_image_using_diffusers_cell_8_output_0.jpeg\" width=\"500\"/> | <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/ghibli-castles.png\" width=\"500\"/> |\n",
    "\n",
    "You can also try experimenting with a different scheduler to see how that affects the output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3607e8",
   "metadata": {
    "id": "yVcKB9u8Ap2T"
   },
   "outputs": [],
   "source": [
    "from diffusers import LMSDiscreteScheduler\n",
    "\n",
    "lms = LMSDiscreteScheduler.from_config(pipe.scheduler.config)\n",
    "pipe.scheduler = lms\n",
    "generator = torch.Generator(device=device).manual_seed(1024)\n",
    "image = pipe(prompt=prompt, image=init_image, strength=0.75, guidance_scale=7.5, generator=generator).images[0]\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9cbd8d",
   "metadata": {
    "id": "pJhbow36Ap2T"
   },
   "source": [
    "### Sample Result\n",
    "| Input                                                                           | Output                                                                                                                                |\n",
    "|---------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| <img src=\"https://huggingface.co/datasets/YiYiXu/test-doc-assets/resolve/main/image_2_image_using_diffusers_cell_8_output_0.jpeg\" width=\"500\"/> | <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/lms-ghibli.png\" width=\"500\"/> |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd816e9d",
   "metadata": {
    "id": "O7Vq4oX4Arna"
   },
   "source": [
    "## Text-guided depth-to-image generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b2689f",
   "metadata": {
    "id": "D95njpBrArnc"
   },
   "source": [
    "The [StableDiffusionDepth2ImgPipeline](https://huggingface.co/docs/diffusers/main/en/api/pipelines/stable_diffusion/depth2img#diffusers.StableDiffusionDepth2ImgPipeline) lets you pass a text prompt and an initial image to condition the generation of new images. In addition, you can also pass a `depth_map` to preserve the image structure. If no `depth_map` is provided, the pipeline automatically predicts the depth via an integrated [depth-estimation model](https://github.com/isl-org/MiDaS).\n",
    "\n",
    "Start by creating an instance of the [StableDiffusionDepth2ImgPipeline](https://huggingface.co/docs/diffusers/main/en/api/pipelines/stable_diffusion/depth2img#diffusers.StableDiffusionDepth2ImgPipeline):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17902ad4",
   "metadata": {
    "id": "1OdobMa_Arnc"
   },
   "outputs": [],
   "source": [
    "pipe = depth_pipe.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fecbd5e9",
   "metadata": {
    "id": "ZnUrjqVtArnc"
   },
   "source": [
    "Now pass your prompt to the pipeline.\n",
    "\n",
    "<Tip>\n",
    "\n",
    "ðŸ’¡ `negative_prompt` prevents certain words from guiding how an image is generated. In the CFG formulation, the subtraction of the unconditional inference is replaced by the negative prompt inference result.\n",
    "    \n",
    "</Tip>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83eb2785",
   "metadata": {
    "id": "MxUknR_eArnc"
   },
   "outputs": [],
   "source": [
    "url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "init_image = Image.open(requests.get(url, stream=True).raw)\n",
    "init_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee21baa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"two tigers\"\n",
    "n_prompt = \"bad, deformed, ugly, bad anatomy\"\n",
    "image = pipe(prompt=prompt, image=init_image, negative_prompt=n_prompt, strength=0.7).images[0]\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c27643",
   "metadata": {
    "id": "DSwG3DulArnd"
   },
   "source": [
    "### Sample Result\n",
    "\n",
    "| Input                                                                           | Output                                                                                                                                |\n",
    "|---------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/coco-cats.png\" width=\"500\"/> | <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/depth2img-tigers.png\" width=\"500\"/> |\n",
    "\n",
    "### Try it yourself!\n",
    "You can try with your custom image url and prompts.\n",
    "\n",
    "Try finding a good balance between image quality and prompt alignment with different parameters. \\\n",
    "You can experiment with different prompts, negative prompts, guidance scale, and noise strength."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdda64ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://images.cocodataset.org/test-stuff2017/000000000509.jpg\"\n",
    "init_image = Image.open(requests.get(url, stream=True).raw)\n",
    "init_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5ed34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"chemistry laboratory\"\n",
    "n_prompt = \"window\"\n",
    "image = pipe(prompt=prompt, image=init_image, negative_prompt=n_prompt, guidance=10, strength=0.95).images[0]\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5ed053",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
