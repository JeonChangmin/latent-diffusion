{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stable Diffusion\n",
    "- [CVPR 2022] High-Resolution Image Synthesis with Latent Diffusion Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "from typing import Any, Dict\n",
    "\n",
    "from einops import rearrange\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![LatentDiffusion](https://kimjy99.github.io/assets/img/ldm/ldm-model.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Predefined Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/changminjeon/.local/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-03-08 06:55:37.516554: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-03-08 06:55:37.560374: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-08 06:55:38.448326: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from ldm.models.autoencoder import AutoencoderKL  # First Stage\n",
    "from ldm.modules.encoders.modules import BERTEmbedder  # Condition Model\n",
    "from ldm.modules.diffusionmodules.openaimodel import UNetModel  # Diffusion Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LatentDiffusion Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LatentDiffusion(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        unet_config,\n",
    "        first_stage_config,\n",
    "        cond_stage_config,\n",
    "        scale_factor=1.0,\n",
    "        timesteps=1000,\n",
    "        linear_start=1e-4,\n",
    "        linear_end=2e-2,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # init models\n",
    "        self.model = UNetModel(**unet_config)\n",
    "        self.first_stage_model = AutoencoderKL(**first_stage_config)\n",
    "        self.cond_stage_model = BERTEmbedder(**cond_stage_config)\n",
    "\n",
    "        # hyperparameters\n",
    "        self.scale_factor = scale_factor\n",
    "        self.num_timesteps = int(timesteps)\n",
    "\n",
    "        ## beta and alpha\n",
    "        betas = (\n",
    "            np.linspace(linear_start**0.5, linear_end**0.5, timesteps, dtype=np.float64)\n",
    "            ** 2\n",
    "        )\n",
    "        alphas = 1.0 - betas\n",
    "        alphas_cumprod = np.cumprod(alphas, axis=0)\n",
    "        alphas_cumprod_prev = np.append(1.0, alphas_cumprod[:-1])\n",
    "        sqrt_one_minus_alphas_cumprod = np.sqrt(1.0 - alphas_cumprod)\n",
    "        assert timesteps == betas.shape[0] == alphas.shape[0] == alphas_cumprod.shape[0]\n",
    "\n",
    "        ## register parameters\n",
    "        to_torch = lambda x: torch.tensor(x, dtype=torch.float32)\n",
    "        self.register_buffer(\"alphas_cumprod\", to_torch(alphas_cumprod))\n",
    "        self.register_buffer(\"alphas_cumprod_prev\", to_torch(alphas_cumprod_prev))\n",
    "        self.register_buffer(\n",
    "            \"sqrt_one_minus_alphas_cumprod\", to_torch(sqrt_one_minus_alphas_cumprod)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, t, cond):\n",
    "        # UNet\n",
    "        return self.model(x, t, context=cond)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DDIM Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDIMSamper:\n",
    "    def __init__(self, model: LatentDiffusion):\n",
    "        self.model = model\n",
    "        self.ddpm_num_timesteps = model.num_timesteps\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def sample(\n",
    "        self,\n",
    "        ddim_steps,\n",
    "        batch_size,\n",
    "        img_shape,\n",
    "        conditioning=None,\n",
    "        unconditional_guidance_scale=1.0,\n",
    "        unconditional_conditioning=None,\n",
    "        temperature=1.0,\n",
    "    ):\n",
    "        # prepare timesteps\n",
    "        ddim_timesteps = (\n",
    "            np.arange(0, self.ddpm_num_timesteps, self.ddpm_num_timesteps // ddim_steps)\n",
    "            + 1\n",
    "        )\n",
    "        device = self.model.alphas_cumprod.device\n",
    "        alphas_cumprod = self.model.alphas_cumprod.cpu()\n",
    "        ddim_alphas = alphas_cumprod[ddim_timesteps]\n",
    "        ddim_alphas_prev = np.hstack([alphas_cumprod[0], alphas_cumprod[ddim_timesteps[:-1]]])\n",
    "        ddim_sqrt_one_minus_alphas = np.sqrt(1.0 - ddim_alphas)\n",
    "        ddim_sigmas = np.zeros(len(ddim_timesteps), dtype=np.float32)\n",
    "\n",
    "        ## convert to torch tensors\n",
    "        to_torch = lambda x: torch.tensor(x, dtype=torch.float32).to(device)\n",
    "        self.ddim_alphas = to_torch(ddim_alphas)\n",
    "        self.ddim_alphas_prev = to_torch(ddim_alphas_prev)\n",
    "        self.ddim_sqrt_one_minus_alphas = to_torch(ddim_sqrt_one_minus_alphas)\n",
    "        self.ddim_sigmas = to_torch(ddim_sigmas)\n",
    "\n",
    "        # sample from noise\n",
    "        samples = torch.randn((batch_size, *img_shape), device=device)\n",
    "        total_steps = len(ddim_timesteps)\n",
    "        iterator = tqdm(\n",
    "            np.flip(ddim_timesteps),\n",
    "            total=total_steps,\n",
    "        )\n",
    "        for i, step in enumerate(iterator):\n",
    "            timestamp = torch.full((batch_size,), step, device=device, dtype=torch.long)\n",
    "            index = total_steps - i - 1\n",
    "            samples = self.sample_ddim(\n",
    "                samples,\n",
    "                conditioning,\n",
    "                timestamp,\n",
    "                index=index,\n",
    "                temperature=temperature,\n",
    "                unconditional_guidance_scale=unconditional_guidance_scale,\n",
    "                unconditional_conditioning=unconditional_conditioning,\n",
    "            )\n",
    "        return samples\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def sample_ddim(\n",
    "        self,\n",
    "        x,\n",
    "        c,\n",
    "        t,\n",
    "        index,\n",
    "        temperature=1.0,\n",
    "        unconditional_guidance_scale=1.0,\n",
    "        unconditional_conditioning=None,\n",
    "    ):\n",
    "        device = x.device\n",
    "        b = x.shape[0]\n",
    "\n",
    "        # handle scale\n",
    "        x_in = torch.cat([x] * 2)\n",
    "        t_in = torch.cat([t] * 2)\n",
    "        c_in = torch.cat([unconditional_conditioning, c])\n",
    "        e_t_uncond, e_t = self.model(x_in, t_in, c_in).chunk(2)\n",
    "        e_t = e_t_uncond + unconditional_guidance_scale * (e_t - e_t_uncond)\n",
    "\n",
    "        alphas = self.ddim_alphas\n",
    "        alphas_prev = self.ddim_alphas_prev\n",
    "        sqrt_one_minus_alphas = self.ddim_sqrt_one_minus_alphas\n",
    "        sigmas = self.ddim_sigmas\n",
    "\n",
    "        # select parameters corresponding to the current timestep\n",
    "        a_t = torch.full((b, 1, 1, 1), alphas[index], device=device)\n",
    "        a_prev = torch.full((b, 1, 1, 1), alphas_prev[index], device=device)\n",
    "        sigma_t = torch.full((b, 1, 1, 1), sigmas[index], device=device)\n",
    "        sqrt_one_minus_at = torch.full(\n",
    "            (b, 1, 1, 1), sqrt_one_minus_alphas[index], device=device\n",
    "        )\n",
    "\n",
    "        # current prediction for x_0\n",
    "        pred_x0 = (x - sqrt_one_minus_at * e_t) / a_t.sqrt()\n",
    "\n",
    "        # direction pointing to x_t\n",
    "        dir_xt = (1.0 - a_prev - sigma_t**2).sqrt() * e_t\n",
    "        noise = sigma_t * torch.randn(x.shape, device=device) * temperature\n",
    "        x_prev = a_prev.sqrt() * pred_x0 + dir_xt + noise\n",
    "        return x_prev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare LatentDiffusion Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Working with z of shape (1, 4, 32, 32) = 4096 dimensions.\n",
      "making attention of type 'vanilla' with 512 in_channels\n"
     ]
    }
   ],
   "source": [
    "with Path(\"stable-diffusion.yaml\").open(\"r\") as f:\n",
    "    model_config = yaml.load(f, Loader=yaml.FullLoader)\n",
    "model = LatentDiffusion(**model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def state_dict_of(state_dict_path: str) -> Dict[str, Any]:\n",
    "    state_dict = torch.load(state_dict_path, map_location=\"cpu\")[\"state_dict\"]\n",
    "    for k in list(state_dict.keys()):\n",
    "        if k.startswith(\"model.diffusion_model.\"):\n",
    "            new_k = k.replace(\"model.diffusion_model.\", \"model.\")\n",
    "            state_dict[new_k] = state_dict.pop(k)\n",
    "    return state_dict\n",
    "\n",
    "\n",
    "missing_keys, unexpected_keys = model.load_state_dict(\n",
    "    state_dict_of(\"/data2/diffusion/stable-diffusion.ckpt\"), strict=False\n",
    ")\n",
    "assert not missing_keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample with LatentDiffusion Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = Namespace()\n",
    "opt.prompt = \"a virus monster is playing guitar, oil on canvas\"\n",
    "opt.n_samples = 4\n",
    "opt.ddim_steps = 50\n",
    "opt.scale = 5.0\n",
    "opt.H = 256\n",
    "opt.W = 256\n",
    "opt.outdir = \"outputs/txt2img-samples\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_558526/2716921192.py:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  to_torch = lambda x: torch.tensor(x, dtype=torch.float32).to(device)\n",
      "100%|██████████| 50/50 [00:09<00:00,  5.40it/s]\n"
     ]
    }
   ],
   "source": [
    "assert torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "sampler = DDIMSamper(model)\n",
    "\n",
    "sample_path = Path(opt.outdir) / \"samples\"\n",
    "sample_path.mkdir(parents=True, exist_ok=True)\n",
    "base_count = len(list(sample_path.glob(\"*.png\")))\n",
    "\n",
    "with torch.no_grad():\n",
    "    uc = model.cond_stage_model.encode(opt.n_samples * [\"\"])\n",
    "    c = model.cond_stage_model.encode(opt.n_samples * [opt.prompt])\n",
    "    img_shape = [4, opt.H // 8, opt.W // 8]\n",
    "    samples_ddim = sampler.sample(\n",
    "        ddim_steps=opt.ddim_steps,\n",
    "        batch_size=opt.n_samples,\n",
    "        img_shape=img_shape,\n",
    "        conditioning=c,\n",
    "        unconditional_guidance_scale=opt.scale,\n",
    "        unconditional_conditioning=uc,\n",
    "    )\n",
    "    samples_ddim = 1.0 / model.scale_factor * samples_ddim\n",
    "    x_samples_ddim = model.first_stage_model.decode(samples_ddim)\n",
    "    x_samples_ddim = torch.clamp((x_samples_ddim + 1.0) / 2.0, min=0.0, max=1.0)\n",
    "\n",
    "    for x_sample in x_samples_ddim:\n",
    "        x_sample = 255.0 * rearrange(x_sample.cpu().numpy(), \"c h w -> h w c\")\n",
    "        Image.fromarray(x_sample.astype(np.uint8)).save(\n",
    "            sample_path / f\"{base_count:04}.png\"\n",
    "        )\n",
    "        base_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ldm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
